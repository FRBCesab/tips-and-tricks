[
  {
    "objectID": "CODE_OF_CONDUCT.html",
    "href": "CODE_OF_CONDUCT.html",
    "title": "Contributor Covenant Code of Conduct",
    "section": "",
    "text": "We as members, contributors, and leaders pledge to make participation in our community a harassment-free experience for everyone, regardless of age, body size, visible or invisible disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, caste, color, religion, or sexual identity and orientation.\nWe pledge to act and interact in ways that contribute to an open, welcoming, diverse, inclusive, and healthy community.\n\n\n\nExamples of behavior that contributes to a positive environment for our community include:\n\nDemonstrating empathy and kindness toward other people\nBeing respectful of differing opinions, viewpoints, and experiences\nGiving and gracefully accepting constructive feedback\nAccepting responsibility and apologizing to those affected by our mistakes, and learning from the experience\nFocusing on what is best not just for us as individuals, but for the overall community\n\nExamples of unacceptable behavior include:\n\nThe use of sexualized language or imagery, and sexual attention or advances of any kind\nTrolling, insulting or derogatory comments, and personal or political attacks\nPublic or private harassment\nPublishing others’ private information, such as a physical or email address, without their explicit permission\nOther conduct which could reasonably be considered inappropriate in a professional setting\n\n\n\n\nCommunity leaders are responsible for clarifying and enforcing our standards of acceptable behavior and will take appropriate and fair corrective action in response to any behavior that they deem inappropriate, threatening, offensive, or harmful.\nCommunity leaders have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, and will communicate reasons for moderation decisions when appropriate.\n\n\n\nThis Code of Conduct applies within all community spaces, and also applies when an individual is officially representing the community in public spaces. Examples of representing our community include using an official e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event.\n\n\n\nInstances of abusive, harassing, or otherwise unacceptable behavior may be reported to the community leaders responsible for enforcement at nicolas.casajus@fondationbiodiversite.fr. All complaints will be reviewed and investigated promptly and fairly.\nAll community leaders are obligated to respect the privacy and security of the reporter of any incident.\n\n\n\nCommunity leaders will follow these Community Impact Guidelines in determining the consequences for any action they deem in violation of this Code of Conduct:\n\n\nCommunity Impact: Use of inappropriate language or other behavior deemed unprofessional or unwelcome in the community.\nConsequence: A private, written warning from community leaders, providing clarity around the nature of the violation and an explanation of why the behavior was inappropriate. A public apology may be requested.\n\n\n\nCommunity Impact: A violation through a single incident or series of actions.\nConsequence: A warning with consequences for continued behavior. No interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, for a specified period of time. This includes avoiding interactions in community spaces as well as external channels like social media. Violating these terms may lead to a temporary or permanent ban.\n\n\n\nCommunity Impact: A serious violation of community standards, including sustained inappropriate behavior.\nConsequence: A temporary ban from any sort of interaction or public communication with the community for a specified period of time. No public or private interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, is allowed during this period. Violating these terms may lead to a permanent ban.\n\n\n\nCommunity Impact: Demonstrating a pattern of violation of community standards, including sustained inappropriate behavior, harassment of an individual, or aggression toward or disparagement of classes of individuals.\nConsequence: A permanent ban from any sort of public interaction within the community.\n\n\n\n\nThis Code of Conduct is adapted from the Contributor Covenant, version 2.1, available at https://www.contributor-covenant.org/version/2/1/code_of_conduct.html.\nCommunity Impact Guidelines were inspired by Mozilla’s code of conduct enforcement ladder.\nFor answers to common questions about this code of conduct, see the FAQ at https://www.contributor-covenant.org/faq. Translations are available at https://www.contributor-covenant.org/translations."
  },
  {
    "objectID": "CODE_OF_CONDUCT.html#our-pledge",
    "href": "CODE_OF_CONDUCT.html#our-pledge",
    "title": "Contributor Covenant Code of Conduct",
    "section": "",
    "text": "We as members, contributors, and leaders pledge to make participation in our community a harassment-free experience for everyone, regardless of age, body size, visible or invisible disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, caste, color, religion, or sexual identity and orientation.\nWe pledge to act and interact in ways that contribute to an open, welcoming, diverse, inclusive, and healthy community."
  },
  {
    "objectID": "CODE_OF_CONDUCT.html#our-standards",
    "href": "CODE_OF_CONDUCT.html#our-standards",
    "title": "Contributor Covenant Code of Conduct",
    "section": "",
    "text": "Examples of behavior that contributes to a positive environment for our community include:\n\nDemonstrating empathy and kindness toward other people\nBeing respectful of differing opinions, viewpoints, and experiences\nGiving and gracefully accepting constructive feedback\nAccepting responsibility and apologizing to those affected by our mistakes, and learning from the experience\nFocusing on what is best not just for us as individuals, but for the overall community\n\nExamples of unacceptable behavior include:\n\nThe use of sexualized language or imagery, and sexual attention or advances of any kind\nTrolling, insulting or derogatory comments, and personal or political attacks\nPublic or private harassment\nPublishing others’ private information, such as a physical or email address, without their explicit permission\nOther conduct which could reasonably be considered inappropriate in a professional setting"
  },
  {
    "objectID": "CODE_OF_CONDUCT.html#enforcement-responsibilities",
    "href": "CODE_OF_CONDUCT.html#enforcement-responsibilities",
    "title": "Contributor Covenant Code of Conduct",
    "section": "",
    "text": "Community leaders are responsible for clarifying and enforcing our standards of acceptable behavior and will take appropriate and fair corrective action in response to any behavior that they deem inappropriate, threatening, offensive, or harmful.\nCommunity leaders have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, and will communicate reasons for moderation decisions when appropriate."
  },
  {
    "objectID": "CODE_OF_CONDUCT.html#scope",
    "href": "CODE_OF_CONDUCT.html#scope",
    "title": "Contributor Covenant Code of Conduct",
    "section": "",
    "text": "This Code of Conduct applies within all community spaces, and also applies when an individual is officially representing the community in public spaces. Examples of representing our community include using an official e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event."
  },
  {
    "objectID": "CODE_OF_CONDUCT.html#enforcement",
    "href": "CODE_OF_CONDUCT.html#enforcement",
    "title": "Contributor Covenant Code of Conduct",
    "section": "",
    "text": "Instances of abusive, harassing, or otherwise unacceptable behavior may be reported to the community leaders responsible for enforcement at nicolas.casajus@fondationbiodiversite.fr. All complaints will be reviewed and investigated promptly and fairly.\nAll community leaders are obligated to respect the privacy and security of the reporter of any incident."
  },
  {
    "objectID": "CODE_OF_CONDUCT.html#enforcement-guidelines",
    "href": "CODE_OF_CONDUCT.html#enforcement-guidelines",
    "title": "Contributor Covenant Code of Conduct",
    "section": "",
    "text": "Community leaders will follow these Community Impact Guidelines in determining the consequences for any action they deem in violation of this Code of Conduct:\n\n\nCommunity Impact: Use of inappropriate language or other behavior deemed unprofessional or unwelcome in the community.\nConsequence: A private, written warning from community leaders, providing clarity around the nature of the violation and an explanation of why the behavior was inappropriate. A public apology may be requested.\n\n\n\nCommunity Impact: A violation through a single incident or series of actions.\nConsequence: A warning with consequences for continued behavior. No interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, for a specified period of time. This includes avoiding interactions in community spaces as well as external channels like social media. Violating these terms may lead to a temporary or permanent ban.\n\n\n\nCommunity Impact: A serious violation of community standards, including sustained inappropriate behavior.\nConsequence: A temporary ban from any sort of interaction or public communication with the community for a specified period of time. No public or private interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, is allowed during this period. Violating these terms may lead to a permanent ban.\n\n\n\nCommunity Impact: Demonstrating a pattern of violation of community standards, including sustained inappropriate behavior, harassment of an individual, or aggression toward or disparagement of classes of individuals.\nConsequence: A permanent ban from any sort of public interaction within the community."
  },
  {
    "objectID": "CODE_OF_CONDUCT.html#attribution",
    "href": "CODE_OF_CONDUCT.html#attribution",
    "title": "Contributor Covenant Code of Conduct",
    "section": "",
    "text": "This Code of Conduct is adapted from the Contributor Covenant, version 2.1, available at https://www.contributor-covenant.org/version/2/1/code_of_conduct.html.\nCommunity Impact Guidelines were inspired by Mozilla’s code of conduct enforcement ladder.\nFor answers to common questions about this code of conduct, see the FAQ at https://www.contributor-covenant.org/faq. Translations are available at https://www.contributor-covenant.org/translations."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "The CESAB – Centre for the Synthesis and Analysis of Biodiversity – is a key program of the FRB (Foundation for Research on Biodiversity) and is an original, innovative and internationally recognized tool that offers researchers a place and time to synthesize and analyze already existing data and information in the field of biodiversity research.\n\n\n\n\n\n\nNoteOpen science\n\n\n\nBy encouraging its groups of researchers to share the tools and databases developed as part of their work, the FRB-CESAB is supporting open science since the centre opened in 2010.\n    Databases       Softwares       Publications       Courses       Tutorials  \nVisit our GitHub organization here!"
  },
  {
    "objectID": "posts/2024-03-11-windows-and-the-home-directory/index.html",
    "href": "posts/2024-03-11-windows-and-the-home-directory/index.html",
    "title": "Windows and the HOME directory",
    "section": "",
    "text": "ImportantWindows-only\n\n\n\nThis post only concerns Windows users."
  },
  {
    "objectID": "posts/2024-03-11-windows-and-the-home-directory/index.html#context",
    "href": "posts/2024-03-11-windows-and-the-home-directory/index.html#context",
    "title": "Windows and the HOME directory",
    "section": "Context",
    "text": "Context\nIn common operating systems, there is an important directory: the HOME directory (also known as the User’s home directory). This directory contains the user’s main folders (Desktop/, Documents/, Pictures/, etc.) but it can also be used by software to store user’s configuration files (ssh, bash, zsh, etc.).\n\n\n\n\n\n\nNoteHome vs. working directory\n\n\n\nThe working directory is the directory from which  was launched. The function getwd() can be run to find this directory. This directory changes between projects. The home directory is user specific and the symbol ~ is often used to refer to this HOME directory.\n\n\n also uses this HOME directory to store two configuration files: the .Renviron and .Rprofile files (run ?Startup to learn more about these files).\nOn Unix systems ( and ), the HOME directory is /home/username (or /Users/username on Apple computers).\nOn Windows , depending on whether you use RStudio IDE (and therefore Rgui.exe) or  in a terminal (and therefore Rterm.exe), the value of this HOME directory can be different:\n\nC:/Users/username on a terminal\nC:/Users/username/Documents on RStudio IDE\n\nAnd the behavior of various  functions is inconsistent. For instance:\n\npath.expand(\"~\")\n## C:/Users/username/Documents\n\nnormalizePath(\"~\")\n## C:\\\\Users\\\\username\\\\Documents\n\nfs::path_home()\n## C:/Users/username"
  },
  {
    "objectID": "posts/2024-03-11-windows-and-the-home-directory/index.html#changing-home-directory",
    "href": "posts/2024-03-11-windows-and-the-home-directory/index.html#changing-home-directory",
    "title": "Windows and the HOME directory",
    "section": "Changing HOME directory",
    "text": "Changing HOME directory\n\n\n\n\n\n\nImportantDanger\n\n\n\nDo not follow this tip if you are several users on the same computer.\n\n\nHere we will resolve the discrepancy between these two different directories by using C:/Users/username everywhere (like in Unix systems).\nAs mentioned in the R for Windows FAQ (section 2.13), we can modify this HOME directory at the system level by setting the environment variable R_USER.\nThis environment variable can be modified by creating a .Renviron.site file in the directory C:/Program Files/R/R-X.X.X/etc/ (where R-X.X.X is the version of ). This file is used to store environment variables at the system level (so for all users).\nN.B. You need to have permission to create/edit this file.\nProceed as follow if the .Renviron.site file does not exist in C:/Program Files/R/R-X.X.X/etc/:\n\nIn C:/Users/username/Desktop/, create an empty file .Renviron.site.\nOpen this file and add this line: R_USER='C:/Users/username' (replace username by your Windows user name).\nSave the file.\nCopy this .Renviron.site file in C:/Program Files/R/R-X.X.X/etc/.\n\nProceed as follow if the .Renviron.site file already exists in C:/Program Files/R/R-X.X.X/etc/:\n\nOpen the .Renviron.site file in C:/Program Files/R/R-X.X.X/etc/ and add this line: R_USER='C:/Users/username' (replace username by your Windows user name).\nSave the file.\n\nAfter restarting , run these lines:\n\nSys.getenv(\"R_USER\")\n## C:/Users/username\n\npath.expand(\"~\")\n## C:/Users/username\n\nnormalizePath(\"~\")\n## C:\\\\Users\\\\username\n\nfs::path_home()\n## C:/Users/username\n\nFrom now you should have the same outputs on RStudio IDE and on a Terminal.\n\n\n\n\n\n\nImportantThis is a temporary fix\n\n\n\nYou will need to repeat this tip each time you reinstall/upgrade ."
  },
  {
    "objectID": "posts/2024-03-11-windows-and-the-home-directory/index.html#edit-20240312",
    "href": "posts/2024-03-11-windows-and-the-home-directory/index.html#edit-20240312",
    "title": "Windows and the HOME directory",
    "section": "Edit (2024/03/12)",
    "text": "Edit (2024/03/12)\nWith some  functions, this tip is not enough. For instance, the functions utils::file.edit(\"~/.Renviron\"), utils::file.edit(\"~/.Rprofile\"), usethis::edit_r_environ(), usethis::edit_r_profile() do not recognized the new HOME directory.\nTo fix this issue, we need to define the R_USER environment variable at the Windows  level.\nProceed as follow:\n\nIn the Windows search bar, type variables and open Modify system environment variables.\nAt the bottom, click on Environment variables…\nIn the section System variable, add a new entry and set:\n\nName: R_USER\nValue: C:\\Users\\username (replace username by your Windows user name)\n\n\nAfter restarting , the HOME directory should always point to C:/Users/username.\n\nSys.getenv(\"R_USER\")\n## C:/Users/username\n\npath.expand(\"~\")\n## C:/Users/username\n\nnormalizePath(\"~\")\n## C:\\\\Users\\\\username\n\nfs::path_home()\n## C:/Users/username"
  },
  {
    "objectID": "posts/2025-01-28-parallel-computing-in-r/index.html",
    "href": "posts/2025-01-28-parallel-computing-in-r/index.html",
    "title": "Parallel Computing in R",
    "section": "",
    "text": "Traditionally, when we work with  we use the sequential computing approach where instructions are processed one at a time, with each subsequent instruction waiting for the previous one to complete. It typically uses a single processor, which can result in lower performance and higher processor workload. The primary drawback of sequential computing is that it can be time-consuming, as only one instruction is executed at any given moment.\nParallel computing was introduced to overcome these limitations. In this approach, multiple instructions or processes are executed concurrently. By running tasks simultaneously, parallel computing saves time and is capable of solving larger problems. It uses multiple high-performance processors, which results in a lower workload for each processor.\n\n\n\nThe Central Processing Unit (CPU), also called processor, is a piece of computer hardware (electronic circuitry) that executes instructions, such as arithmetic, logic, controlling, and input/output (I/O) operations1. In other words, it is the brain of the computer.\nModern CPUs consist of several units, named (physical) cores (multi-core processors). These cores can be multithreaded. This means that these cores can provide multiple threads of execution in parallel (usually up to 2). Threads are also known as logical cores.\n\n\n\n\n\nIn this example, the CPU (in brown) contains 4 cores (in blue and green):\n\n2 single-threading cores (in blue)\n2 multi-threading cores (in green)\n\nSix threads (logical cores) are available for parallel computing.\n\n\n\n\n\n\n\n\nNote\n\n\n\nFor instance, my Lenovo P14s Gen 3 is shipped with the Intel® Core™ i7-1280P processor. It is made up of 6 multi-threading cores (2 threads per core) and 8 single-threading cores. Parallel computation can use 20 threads.\n\n\n\n# How many logical cores are available on my computer?\nparallel::detectCores(logical = TRUE)\n## [1] 20\n\n\n\n\n\nHere we will focus on the Embarrassingly parallel computing paradigm. In this paradigm, a problem can be split into multiple independent pieces. The pieces of the problem are executed simultaneously as they don’t have to communicate with each other (except at the end when all outputs are assembled).\n\n\n\n\n\n\nImportantHidden parallel tasks\n\n\n\nMany  packages (and system libraries) include built-in parallel computing that operates behind the scenes. This hidden parallel computing won’t interfere with your work and will actually enhance computational efficiency. However, it’s still a good idea to be aware of it, as it could impact other tasks (and users) using the same machine.\n\n\n  Data parallel computing involves splitting a large dataset into smaller segments and performing the same operation on each segment simultaneously. This approach is especially useful for tasks where the same computation must be applied to multiple data points (e.g. large data processing, simulations, machine learning, image and video processing).\nMany  packages implement high-performance and parallel computing (see this CRAN Task View).\nThe parallel package in  implements two types of parallel computing:\n\nShared memory parallelization (or FORK): Each parallel thread is essentially a full copy of the master process, along with the shared environment, including objects and variables defined before the parallel threads are launched. This makes it run efficiently, especially when handling large datasets. However, a key limitation is that the approach does not work on Windows systems.\nDistributed memory parallelization (or SOCKET): Each thread operates independently, without sharing objects or variables, which must be explicitly passed from the master process. As a result, it tends to run slower due to the overhead of communication. This approach is compatible with all operating systems.\n\n\nTable: Comparison of two parallel computing approaches\n\n\n\n\n\n\n\n\nForking\nSocket\n\n\n\n\nOperating system\n   \n      \n\n\nEnvironment\nCommon to all sessions(time saving)\nUnique to each session(transfer variables, functions & packages)\n\n\nUsage\nVery easy\nMore difficult(configure cluster & environment)\n\n\n\n\nSource (in French): https://regnault.pages.math.cnrs.fr/meroo/src/quarto/calc_paral_R.html\n\nLet’s explore these two approaches with a case study."
  },
  {
    "objectID": "posts/2025-01-28-parallel-computing-in-r/index.html#introduction",
    "href": "posts/2025-01-28-parallel-computing-in-r/index.html#introduction",
    "title": "Parallel Computing in R",
    "section": "",
    "text": "Traditionally, when we work with  we use the sequential computing approach where instructions are processed one at a time, with each subsequent instruction waiting for the previous one to complete. It typically uses a single processor, which can result in lower performance and higher processor workload. The primary drawback of sequential computing is that it can be time-consuming, as only one instruction is executed at any given moment.\nParallel computing was introduced to overcome these limitations. In this approach, multiple instructions or processes are executed concurrently. By running tasks simultaneously, parallel computing saves time and is capable of solving larger problems. It uses multiple high-performance processors, which results in a lower workload for each processor.\n\n\n\nThe Central Processing Unit (CPU), also called processor, is a piece of computer hardware (electronic circuitry) that executes instructions, such as arithmetic, logic, controlling, and input/output (I/O) operations1. In other words, it is the brain of the computer.\nModern CPUs consist of several units, named (physical) cores (multi-core processors). These cores can be multithreaded. This means that these cores can provide multiple threads of execution in parallel (usually up to 2). Threads are also known as logical cores.\n\n\n\n\n\nIn this example, the CPU (in brown) contains 4 cores (in blue and green):\n\n2 single-threading cores (in blue)\n2 multi-threading cores (in green)\n\nSix threads (logical cores) are available for parallel computing.\n\n\n\n\n\n\n\n\nNote\n\n\n\nFor instance, my Lenovo P14s Gen 3 is shipped with the Intel® Core™ i7-1280P processor. It is made up of 6 multi-threading cores (2 threads per core) and 8 single-threading cores. Parallel computation can use 20 threads.\n\n\n\n# How many logical cores are available on my computer?\nparallel::detectCores(logical = TRUE)\n## [1] 20\n\n\n\n\n\nHere we will focus on the Embarrassingly parallel computing paradigm. In this paradigm, a problem can be split into multiple independent pieces. The pieces of the problem are executed simultaneously as they don’t have to communicate with each other (except at the end when all outputs are assembled).\n\n\n\n\n\n\nImportantHidden parallel tasks\n\n\n\nMany  packages (and system libraries) include built-in parallel computing that operates behind the scenes. This hidden parallel computing won’t interfere with your work and will actually enhance computational efficiency. However, it’s still a good idea to be aware of it, as it could impact other tasks (and users) using the same machine.\n\n\n  Data parallel computing involves splitting a large dataset into smaller segments and performing the same operation on each segment simultaneously. This approach is especially useful for tasks where the same computation must be applied to multiple data points (e.g. large data processing, simulations, machine learning, image and video processing).\nMany  packages implement high-performance and parallel computing (see this CRAN Task View).\nThe parallel package in  implements two types of parallel computing:\n\nShared memory parallelization (or FORK): Each parallel thread is essentially a full copy of the master process, along with the shared environment, including objects and variables defined before the parallel threads are launched. This makes it run efficiently, especially when handling large datasets. However, a key limitation is that the approach does not work on Windows systems.\nDistributed memory parallelization (or SOCKET): Each thread operates independently, without sharing objects or variables, which must be explicitly passed from the master process. As a result, it tends to run slower due to the overhead of communication. This approach is compatible with all operating systems.\n\n\nTable: Comparison of two parallel computing approaches\n\n\n\n\n\n\n\n\nForking\nSocket\n\n\n\n\nOperating system\n   \n      \n\n\nEnvironment\nCommon to all sessions(time saving)\nUnique to each session(transfer variables, functions & packages)\n\n\nUsage\nVery easy\nMore difficult(configure cluster & environment)\n\n\n\n\nSource (in French): https://regnault.pages.math.cnrs.fr/meroo/src/quarto/calc_paral_R.html\n\nLet’s explore these two approaches with a case study."
  },
  {
    "objectID": "posts/2025-01-28-parallel-computing-in-r/index.html#case-study",
    "href": "posts/2025-01-28-parallel-computing-in-r/index.html#case-study",
    "title": "Parallel Computing in R",
    "section": "Case study",
    "text": "Case study\n  Objective: We want to intersect the spatial distribution of 40 species on a spatial grid (defining the Western Palearctic region) to create a matrix with grid cells in row and the presence/absence of each species in column.\nThe workflow is the following:\n\nImport study area (grid)\nImport species spatial distributions (polygons)\nSubset polygons for species X\nIntersect polygons X with the grid\nAssemble occurrences of all species\n\nSteps 3-4 must be repeated for each species and therefore can be parallelized.\n\n\nImport data\nLet’s import the study area, a regular spatial grid (sf POLYGONS) defined in the WGS84 system and delimiting the Western Palearctic region.\n\n# Import study area ----\nstudy_area  &lt;- sf::st_read(file.path(\"data\", \"study_area.gpkg\"))\nstudy_area\n\n\n\nSimple feature collection with 3254 features and 1 field\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -24.53429 ymin: 14.91196 xmax: 69.02288 ymax: 81.85871\nGeodetic CRS:  WGS 84\nFirst 10 features:\n   id                           geom\n1   1 MULTIPOLYGON (((11.46571 15...\n2   2 MULTIPOLYGON (((11.46571 15...\n3   3 MULTIPOLYGON (((12.46571 15...\n4   4 MULTIPOLYGON (((13.46571 15...\n5   5 MULTIPOLYGON (((14.46571 15...\n6   6 MULTIPOLYGON (((15.46571 15...\n7   7 MULTIPOLYGON (((16.46571 15...\n8   8 MULTIPOLYGON (((17.46571 15...\n9   9 MULTIPOLYGON (((26.46571 15...\n10 10 MULTIPOLYGON (((26.46571 15...\n\n\n\n# Attach ggplot2 ----\nlibrary(\"ggplot2\")\n\n# Map study area ----\nggplot() +\n  theme_bw() +\n  theme(text = element_text(family = \"serif\")) +\n  geom_sf(data = study_area)\n\n\n\n\n\n\n\n\nNow let’s import the bird spatial distribution layer (sf POLYGONS) also defined in the WGS84 system.\n\n# Import species distribution ----\nsp_polygons &lt;- sf::st_read(file.path(\"data\", \"species_polygons.gpkg\"))\nsp_polygons\n\n\n\nSimple feature collection with 40 features and 1 field\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -30 ymin: 10 xmax: 75 ymax: 81.85748\nGeodetic CRS:  WGS 84\nFirst 10 features:\n     binomial                           geom\n1  species_01 MULTIPOLYGON (((6.04248 45....\n2  species_02 MULTIPOLYGON (((-16.16014 2...\n3  species_03 MULTIPOLYGON (((29.5531 40....\n4  species_04 MULTIPOLYGON (((75 12.38982...\n5  species_05 MULTIPOLYGON (((-7.675472 4...\n6  species_06 MULTIPOLYGON (((-3.947759 4...\n7  species_07 MULTIPOLYGON (((4.209473 36...\n8  species_08 MULTIPOLYGON (((75 42.27599...\n9  species_09 MULTIPOLYGON (((-4.051088 4...\n10 species_10 MULTIPOLYGON (((5.534302 61...\n\n\n\n\n\nExtract species names\nLet’s extract the name of the 40 species (used later for parallel computing).\n\n# Extract species names ----\nsp_names &lt;- unique(sp_polygons$\"binomial\")\nsp_names &lt;- sort(sp_names)\nsp_names\n\n [1] \"species_01\" \"species_02\" \"species_03\" \"species_04\" \"species_05\"\n [6] \"species_06\" \"species_07\" \"species_08\" \"species_09\" \"species_10\"\n[11] \"species_11\" \"species_12\" \"species_13\" \"species_14\" \"species_15\"\n[16] \"species_16\" \"species_17\" \"species_18\" \"species_19\" \"species_20\"\n[21] \"species_21\" \"species_22\" \"species_23\" \"species_24\" \"species_25\"\n[26] \"species_26\" \"species_27\" \"species_28\" \"species_29\" \"species_30\"\n[31] \"species_31\" \"species_32\" \"species_33\" \"species_34\" \"species_35\"\n[36] \"species_36\" \"species_37\" \"species_38\" \"species_39\" \"species_40\"\n\n\n\n\n\nExplore data\nWe will select polygons for species_22 and map layers.\n\n# Subset one species ----\nspecies      &lt;- \"species_22\"\nsub_polygons &lt;- sp_polygons[sp_polygons$\"binomial\" == species, ]\nsub_polygons\n\nSimple feature collection with 1 feature and 1 field\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -6.235596 ymin: 52.44647 xmax: 34.74487 ymax: 71.18811\nGeodetic CRS:  WGS 84\n     binomial                           geom\n22 species_22 MULTIPOLYGON (((5.123108 60...\n\n\n\n# Map layers ----\nggplot() +\n  theme_bw() +\n  theme(text = element_text(family = \"serif\")) +\n  geom_sf(data = study_area) +\n  geom_sf(data = sub_polygons, \n          fill = \"#4F000088\", \n          col  = \"#4F0000FF\")\n\n\n\n\n\n\n\n\n\n\n\nDefine main function\nNow let’s define a function that will report the occurrence of one species on the grid (step 4 of the workflow).\n\n# Function to rasterize ----\npolygon_to_grid &lt;- function(grid, polygon) {\n\n  ## Clean species name ----\n  species &lt;- unique(polygon$\"binomial\")\n  species &lt;- gsub(\" \", \"_\", species) |&gt; \n    tolower()\n  \n  ## Intersect layers ----\n  cells &lt;- sf::st_intersects(grid, polygon, sparse = FALSE)\n  cells &lt;- apply(cells, 1, any)\n  cells &lt;- which(cells)\n  \n  ## Create presence/absence column ----\n  grid[     , species] &lt;- 0\n  grid[cells, species] &lt;- 1\n  \n  ## Remove spatial information ----\n  sf::st_drop_geometry(grid)\n}\n\nThis function returns a data.frame with two columns: the identifier of the cell and the presence/absence of the species.\n\n\n\nTest main function\nLet’s try this function with the distribution of Curruca cantillans.\n\n# Rasterize species polygons -----\nsp_grid &lt;- polygon_to_grid(study_area, sub_polygons)\n\nalthough coordinates are longitude/latitude, st_intersects assumes that they\nare planar\n\nhead(sp_grid)\n\n  id species_22\n1  1          0\n2  2          0\n3  3          0\n4  4          0\n5  5          0\n6  6          0\n\n\n\n# Add geometry ----\nsf::st_geometry(sp_grid) &lt;- sf::st_geometry(study_area)\n\n# Convert occurrence to factor ----\nsp_grid$\"species_22\" &lt;- as.factor(sp_grid$\"species_22\")\n\n# Map result ----\nggplot() +\n  theme_bw() +\n  theme(text = element_text(family = \"serif\"), legend.position = \"none\") +\n  geom_sf(data    = sp_grid, \n          mapping = aes(fill = species_22)) +\n  scale_fill_manual(values = c(\"0\" = \"#FFFFFFFF\", \n                               \"1\" = \"#9F0000FF\"))\n\n\n\n\n\n\n\n\nOur workflow is ready for one species. Now we have to apply this function on each species. Let’s have a look at different approaches.\n\n\n\nNon-parallel: for() loop\nOne way of repeating a task is the iteration. Let’s walk over species with the for() loop.\n\n# Create the output (list of 40 empty elements) ----\ngrids &lt;- vector(mode = \"list\", length = length(sp_names))\n\n# Define the sequence to loop on ----\nfor (i in 1:length(sp_names)) {\n  \n  # Instructions to repeat ----\n  tmp &lt;- sp_polygons[sp_polygons$\"binomial\" == sp_names[i], ]\n  grd &lt;- polygon_to_grid(study_area, tmp)\n  \n  # Store result in the output ----\n  grids[[i]] &lt;- grd\n\n}\n\nThe output is a list of 40 data.frame.\n\n\n\nNon-parallel: lapply()\nIterative computing is quite verbose and often time-consuming. Because  is a functional programming language, we can wrap-up the body of the for() loop inside a function and apply this function over a vector (i.e. species names). Let’s illustrate this with the function lapply().\n\n# lapply way ----\ngrids &lt;- lapply(sp_names, function(sp) {\n  \n  tmp &lt;- sp_polygons[sp_polygons$\"binomial\" == sp, ]\n  grd &lt;- polygon_to_grid(study_area, tmp)\n  grd\n\n})\n\nThe function lapply() also returns a list of 40 data.frame.\n\n\n\n\n\n\nTipThe purrr package\n\n\n\nThe purrr package provides an alternative to the apply() function family in base . For instance:\n\n# purrr way ----\ngrids &lt;- purrr::map(sp_names, function(sp) {\n  \n  tmp &lt;- sp_polygons[sp_polygons$\"binomial\" == sp, ]\n  grd &lt;- polygon_to_grid(study_area, tmp)\n  grd\n\n})\n\nThe map() function of the purrr package returns the same output as lapply().\n\n\nThese three approaches work in a sequential way. It’s time to parallel our code to boost performance.\n\n\n\nForking: mclapply()\nThe parallel package provides the function mclapply(), a parallelized version of lapply(). Note that this function relies on the forking approach and doesn’t work on Windows.\n\n# Required packages ----\nlibrary(parallel)\n\n# Let's use 10 threads ----\nn_cores &lt;- 10\n\n# mclapply way ----\ngrids &lt;- mclapply(sp_names, function(sp) {\n  \n  tmp &lt;- sp_polygons[sp_polygons$\"binomial\" == sp, ]\n  grd &lt;- polygon_to_grid(study_area, tmp)\n  grd\n\n}, mc.cores = n_cores)\n\nThe argument mc.cores is used to indicate the number of threads to use. The mclapply() will always return a list (as lapply()).\n  If you are on Unix-based systems (GNU/Linux or macOS), you should use the forking approach.\n\n\n\nSocket: parLapply()\nThe socket approach is available for all operating systems (including Windows). The parLapply() function is the equivalent to the mclapply() but additional actions are required to parallelize code under the socket approach:\n\nCreate a socket with the required number of threads with makeCluster()\nTransfer the environment: packages with clusterEvalQ() and variables/functions in memory with clusterExport()\nStop the socket at the end of the parallel computation with stopCluster()\n\n\n# Required packages ----\nlibrary(parallel)\n\n# Let's use 10 threads ----\nn_cores &lt;- 10\n\n# Create a socket w/ 10 threads ----\ncluster &lt;- makeCluster(spec = n_cores)\n\n# Attach packages in each session ----\nclusterEvalQ(cl   = cluster, \n             expr = { \n  library(sf) \n})\n\n# Transfer data & functions to each session ----\nclusterExport(cl      = cluster, \n              varlist = c(\"sp_names\", \"sp_polygons\", \"study_area\", \n                          \"polygon_to_grid\"),\n              envir   = environment())\n\n# Parallel computing ----\ngrids &lt;- parLapply(cl = cluster, X = sp_names, function(sp) {\n  \n  tmp &lt;- sp_polygons[sp_polygons$\"binomial\" == sp, ]\n  grd &lt;- polygon_to_grid(study_area, tmp)\n  grd\n  \n})\n\n# Stop socket ----\nstopCluster(cluster)\n\nThe parLapply() also returns a list of 40 data.frame.\n\n\n\nSocket: foreach()\nInstead of the parLapply() you can use the foreach() function of the foreach package. But before parallelizing the code, you will need to register the socket with the registerDoParallel() function of the package doParallel.\n\n# Required packages ----\nlibrary(parallel)\nlibrary(foreach)\nlibrary(doParallel)\n\n# Let's use 10 threads ----\nn_cores &lt;- 10\n\n# Create a socket w/ 10 threads ----\ncluster &lt;- makeCluster(spec = n_cores)\n\n# Attach packages in each session ----\nclusterEvalQ(cl   = cluster, \n             expr = { \n  library(sf) \n})\n\n# Transfer data & functions to each session ----\nclusterExport(cl      = cluster, \n              varlist = c(\"sp_names\", \"sp_polygons\", \"study_area\", \n                          \"polygon_to_grid\"),\n              envir   = environment())\n\n# Register socket ----\nregisterDoParallel(cluster)\n\n# Parallel computing ----\ngrids &lt;- foreach(i = 1:length(sp_names), .combine = list) %dopar% {\n  \n  tmp &lt;- sp_polygons[sp_polygons$\"binomial\" == sp_names[i], ]\n  grd &lt;- polygon_to_grid(study_area, tmp)\n  grd\n}\n\n# Stop socket ----\nstopCluster(cluster)\n\nThe argument .combine of the foreach() function can be used to specify how the individual results are combined together. By default the function returns a list.\n\n\n\nBenchmark\nLet’s compare the speed of each methods with the system.time() function.\n\n# Required packages ----\nlibrary(parallel)\nlibrary(foreach)\nlibrary(doParallel)\n\n# Let's use 10 threads ----\nn_cores &lt;- 10\n\n# for loop ----\nfor_loop &lt;- system.time({\n  grids &lt;- vector(mode = \"list\", length = length(sp_names))\n  for (i in 1:length(sp_names)) {\n    tmp &lt;- sp_polygons[sp_polygons$\"binomial\" == sp_names[i], ]\n    grids[[i]] &lt;- polygon_to_grid(study_area, tmp)\n  }\n})\n\n\n# lapply way ----\nlapply_way &lt;- system.time({\n  lapply(sp_names, function(sp) {\n    tmp &lt;- sp_polygons[sp_polygons$\"binomial\" == sp, ]\n    polygon_to_grid(study_area, tmp)\n  })\n})\n\n\n# purrr way ----\npurrr_way &lt;- system.time({\n  purrr::map(sp_names, function(sp) {\n    tmp &lt;- sp_polygons[sp_polygons$\"binomial\" == sp, ]\n    polygon_to_grid(study_area, tmp)\n  })\n})\n\n\n# mclapply way ----\nmclapply_way &lt;- system.time({\n  mclapply(sp_names, function(sp) {\n    tmp &lt;- sp_polygons[sp_polygons$\"binomial\" == sp, ]\n    polygon_to_grid(study_area, tmp)\n  }, \n  mc.cores = n_cores)\n})\n\n\n# parlapply way ----\nparlapply_way &lt;- system.time({\n  cluster &lt;- makeCluster(spec = n_cores)\n  clusterEvalQ(cl = cluster, expr = { library(sf) })\n  clusterExport(cl      = cluster, \n                varlist = c(\"sp_names\", \"sp_polygons\", \n                            \"study_area\", \"polygon_to_grid\"),\n                envir   = environment())\n  parLapply(cl = cluster, sp_names, function(sp) {\n    tmp &lt;- sp_polygons[sp_polygons$\"binomial\" == sp, ]\n    polygon_to_grid(study_area, tmp)\n  })\n  stopCluster(cluster)\n})\n\n\n# foreach way ----\nforeach_way &lt;- system.time({\n  cluster &lt;- makeCluster(spec = n_cores)\n  clusterEvalQ(cl = cluster, expr = { library(sf) })\n  clusterExport(cl      = cluster, \n                varlist = c(\"sp_names\", \"sp_polygons\", \"study_area\", \n                            \"polygon_to_grid\"),\n                envir   = environment())\n  registerDoParallel(cluster)\n  foreach(i = 1:length(sp_names), .combine = list) %dopar% {\n    tmp &lt;- sp_polygons[sp_polygons$\"binomial\" == sp_names[i], ]\n    polygon_to_grid(study_area, tmp)\n  }\n  stopCluster(cluster)\n})\n\n\n# Benchmark ----\nrbind(for_loop, lapply_way, purrr_way, \n      mclapply_way, parlapply_way, foreach_way)\n\n              elapsed\nfor_loop       12.488\nlapply_way     12.369\npurrr_way      12.367\nforeach_way     3.650\nparlapply_way   3.348\nmclapply_way    2.246\nAs we can see, parallelizing portions of code is very efficient. In this example, both fork and socket approaches are quite fast. But differences in performance may appear depending on your code (and your available memory).\n\n\n\nBonus: aggregate outputs\nFinally, let’s aggregate all data.frame stored in the list into a single spatial object.\n\n# Parallel computing (forking) ----\ngrids &lt;- parallel::mclapply(sp_names, function(sp) {\n  \n  tmp &lt;- sp_polygons[sp_polygons$\"binomial\" == sp, ]\n  polygon_to_grid(study_area, tmp)\n  \n}, mc.cores = n_cores)\n\n# Aggregation ----\ngrids &lt;- do.call(cbind, grids)\n\n# Keep only species columns ----\ngrids &lt;- grids[ , which(colnames(grids) != \"id\")]\n\n# Convert to spatial object ----\nsf::st_geometry(grids) &lt;- sf::st_geometry(study_area)\n\n\n\n\nTo go further\n\nIntroduction to parallel computing with R - link\nCalcul parallèle avec R [in French] - link\nBoosting R performance with parallel processing package snow - link\nfuture: Unified parallel and distributed processing in R for everyone - link\nQuick introduction to parallel computing in R - link\nParalléliser R [in French] - link\nParallel computation - link"
  },
  {
    "objectID": "posts/2025-01-28-parallel-computing-in-r/index.html#footnotes",
    "href": "posts/2025-01-28-parallel-computing-in-r/index.html#footnotes",
    "title": "Parallel Computing in R",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nhttps://en.wikipedia.org/wiki/Central_processing_unit↩︎"
  },
  {
    "objectID": "posts/2024-02-27-rstudio-shortcuts-a-selection/index.html",
    "href": "posts/2024-02-27-rstudio-shortcuts-a-selection/index.html",
    "title": "RStudio shortcuts: a selection",
    "section": "",
    "text": "This post lists useful shortcuts that will increase your productivity in using RStudio IDE. This subset is a subjective selection: for more shortcuts, press the combination Alt + Shift + K.\n\nTable 1. A subjective selection of RStudio shortcuts\n\n\n\n\n\n\n\nCategory\nAction\nKey combination\n\n\n\n\nNavigation\nGo to next tab\nGo to previous tab\nCtrl + Tab\nCtrl + Shift + Tab\n\n\nPanels\nMove cursor to Editor\nMove cursor to Console\nMove cursor to Terminal\nExpand Editor panel\nExpand Console panel\nCtrl + 1\nCtrl + 2\nAlt + Shift + M\nCtrl + Shift + 1\nCtrl + Shift + 2\n\n\nFiles\nCreate a new .R file\nSave file\nClose file\nCtrl + Shift + N\nCtrl + S\nCtrl + W\n\n\nEditor\nComment/uncomment current line/selection\nSend current line/selection to Console\nSend current line/selection to Terminal\nInsert assignment operator (&lt;-)\nInsert pipe operator (%&gt;% or |&gt;)\nMove current line up\nMove current line down\nDelete current line\nDuplicate current line (below)\nYank (cut) line up to cursor\nYank (cut) line after cursor\nAdd a cursor above current cursor\nAdd a cursor below current cursor\nAdd a cursor on the click\nCtrl + Shift + C\nCtrl + Return\nCtrl + Shift + Return\nCtrl + Shift + +\nCtrl + Shift + Return\nAlt + Up\nAlt + Down\nCtrl + D\nCtrl + Shift + D\nCtrl + U\nCtrl + K\nCtrl + Alt + Up\nCtrl + Alt + Down\nCtrl + Alt + Click\n\n\nBuild\nLoad project (devtools::load_all())\nCheck package (devtools::check())\nKnit/Render/Preview .(R|q)md files\nInsert code chunk in .(R|q)md files\nCtrl + Shift + L\nCtrl + Shift + E\nCtrl + Shift + K\nCtrl + Alt + I\n\n\nOther\nClear console/terminal\nCtrl + L"
  },
  {
    "objectID": "posts/2024-11-19-the-apply-function-family/index.html",
    "href": "posts/2024-11-19-the-apply-function-family/index.html",
    "title": "The apply() function family",
    "section": "",
    "text": "In this post, we will discuss about the family of apply() functions. These functions allows you to recursively apply a function across all elements of a vector, list, matrix, or data.frame. The apply() family is an interesting alternative to the for loop because it wraps the loop into a simple function.\nThe functions in the apply() family differ in their input and output types:\nNB. Here we won’t talk about sapply() and vapply() as there are similar to lapply()."
  },
  {
    "objectID": "posts/2024-11-19-the-apply-function-family/index.html#dataset",
    "href": "posts/2024-11-19-the-apply-function-family/index.html#dataset",
    "title": "The apply() function family",
    "section": "Dataset",
    "text": "Dataset\nTo illustrate to use of apply() functions, we will use the palmerpenguins package. It contains the penguins dataset with size measurements for three penguin species observed on three islands in the Palmer Archipelago, Antarctica.\n\n\n\nAtwork by Allison Horst\n\n\n\nThese data were collected from 2007 and 2009 by Dr. Kristen Gorman and are released under the CC0 license.\n\n\nLet’s install the released version of palmerpenguins package from CRAN:\n\n## Install 'palmerpenguins' package ----\ninstall.packages(\"palmerpenguins\")\n\nNow, let’s import the dataset:\n\n## Import 'penguins' dataset ----\nlibrary(\"palmerpenguins\")\n\n\nAttaching package: 'palmerpenguins'\n\n\nThe following objects are masked from 'package:datasets':\n\n    penguins, penguins_raw\n\npenguins\n\n# A tibble: 344 × 8\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Adelie  Torgersen           39.1          18.7               181        3750\n 2 Adelie  Torgersen           39.5          17.4               186        3800\n 3 Adelie  Torgersen           40.3          18                 195        3250\n 4 Adelie  Torgersen           NA            NA                  NA          NA\n 5 Adelie  Torgersen           36.7          19.3               193        3450\n 6 Adelie  Torgersen           39.3          20.6               190        3650\n 7 Adelie  Torgersen           38.9          17.8               181        3625\n 8 Adelie  Torgersen           39.2          19.6               195        4675\n 9 Adelie  Torgersen           34.1          18.1               193        3475\n10 Adelie  Torgersen           42            20.2               190        4250\n# ℹ 334 more rows\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\nFor this post, we will use a subset of this dataset:\n\n## Columns to keep ----\ncols &lt;- c(\"species\", \"island\", \"bill_length_mm\", \"bill_depth_mm\", \n          \"body_mass_g\")\n\n## Subset data ----\npenguins &lt;- penguins[ , cols]\npenguins\n\n# A tibble: 344 × 5\n   species island    bill_length_mm bill_depth_mm body_mass_g\n   &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;       &lt;int&gt;\n 1 Adelie  Torgersen           39.1          18.7        3750\n 2 Adelie  Torgersen           39.5          17.4        3800\n 3 Adelie  Torgersen           40.3          18          3250\n 4 Adelie  Torgersen           NA            NA            NA\n 5 Adelie  Torgersen           36.7          19.3        3450\n 6 Adelie  Torgersen           39.3          20.6        3650\n 7 Adelie  Torgersen           38.9          17.8        3625\n 8 Adelie  Torgersen           39.2          19.6        4675\n 9 Adelie  Torgersen           34.1          18.1        3475\n10 Adelie  Torgersen           42            20.2        4250\n# ℹ 334 more rows"
  },
  {
    "objectID": "posts/2024-11-19-the-apply-function-family/index.html#the-apply-function",
    "href": "posts/2024-11-19-the-apply-function-family/index.html#the-apply-function",
    "title": "The apply() function family",
    "section": "The apply() function",
    "text": "The apply() function\nThe apply() lets you perform a function across rows or columns of a data.frame (or any types of 2-dimension objects).\n\nthe first argument X specifies the data\nthe second argument MARGIN specifies the direction (1 for rows, 2 for columns)\nthe third argument FUN is the function to apply\n\nLet’s compute the arithmetic mean of the columns bill_length_mm, bill_depth_mm and body_mass_g by applying the mean() function across columns 3 to 5 of the penguins dataset.\n\n## Mean of columns 3, 4 and 5 ----\napply(penguins[ , 3:5], 2, mean)\n\nbill_length_mm  bill_depth_mm    body_mass_g \n            NA             NA             NA \n\n\nWe can pass arguments to the function mean() by using the argument ... of the function apply(). Let’s remove missing values before computing the mean.\n\n## Use additional arguments ----\napply(penguins[ , 3:5], 2, mean, na.rm = TRUE)\n\nbill_length_mm  bill_depth_mm    body_mass_g \n      43.92193       17.15117     4201.75439 \n\n\nNote that the apply() functions are pipe-friendly.\n\n## Pipe version ----\npenguins[ , 3:5] |&gt; \n  apply(2, mean, na.rm = TRUE)\n\nbill_length_mm  bill_depth_mm    body_mass_g \n      43.92193       17.15117     4201.75439 \n\n\nWe can also use a custom function.\n\n## Custom function ----\napply(penguins[ , 3:5], 2, function(x) mean(x, na.rm = TRUE))\n\nbill_length_mm  bill_depth_mm    body_mass_g \n      43.92193       17.15117     4201.75439 \n\n\nFinally, we can define a custom function outside the apply() function.\n\n## Custom function ----\nmy_mean &lt;- function(x, na_rm = FALSE) {\n  mean(x, na.rm = na_rm)\n}\n\napply(penguins[ , 3:5], 2, my_mean, na_rm = TRUE)\n\nbill_length_mm  bill_depth_mm    body_mass_g \n      43.92193       17.15117     4201.75439 \n\n\nThe output is a vector, but in some cases it can be a matrix (or an array).\n\n## Different output class ----\napply(penguins[ , 3:5], 2, range, na.rm = TRUE)\n\n     bill_length_mm bill_depth_mm body_mass_g\n[1,]           32.1          13.1        2700\n[2,]           59.6          21.5        6300"
  },
  {
    "objectID": "posts/2024-11-19-the-apply-function-family/index.html#the-lapply-function",
    "href": "posts/2024-11-19-the-apply-function-family/index.html#the-lapply-function",
    "title": "The apply() function family",
    "section": "The lapply() function",
    "text": "The lapply() function\nThe lapply() function performs a function on each element of a list or vector.\n\nthe first argument X specifies the list or the vector\nthe second argument FUN is the function to apply\n\nLet’s try to compute the arithmetic mean of the columns bill_length_mm, bill_depth_mm and body_mass_g.\n\n## Column names ----\ncolumns &lt;- c(\"bill_length_mm\", \"bill_depth_mm\", \"body_mass_g\")\n\n## Mean of columns 3, 4 and 5 ----\nlapply(columns, function(x) {\n  penguins[ , x, drop = TRUE] |&gt; \n    mean(na.rm = TRUE)\n})\n\n[[1]]\n[1] 43.92193\n\n[[2]]\n[1] 17.15117\n\n[[3]]\n[1] 4201.754\n\n\nThe output is a list of same length as X, and we can simplified it by using unlist(). We can do this because the output for each iteration is a single value.\n\n## Column names ----\ncolumns &lt;- c(\"bill_length_mm\", \"bill_depth_mm\", \"body_mass_g\")\n\n## Mean of columns 3, 4 and 5 ----\nvalues &lt;- lapply(columns, function(x) {\n  penguins[ , x, drop = TRUE] |&gt; \n    mean(na.rm = TRUE)\n})\n\n## Simplify output ----\nunlist(values)\n\n[1]   43.92193   17.15117 4201.75439\n\n\nAnd we can name values.\n\n## Column names ----\ncolumns &lt;- c(\"bill_length_mm\", \"bill_depth_mm\", \"body_mass_g\")\n\n## Mean of columns 3, 4 and 5 ----\nvalues &lt;- lapply(columns, function(x) {\n  penguins[ , x, drop = TRUE] |&gt; \n    mean(na.rm = TRUE)\n})\n\n## Simplify output ----\nvalues &lt;- unlist(values)\n\n## Name elements ----\nnames(values) &lt;- columns\n\nvalues\n\nbill_length_mm  bill_depth_mm    body_mass_g \n      43.92193       17.15117     4201.75439 \n\n\nThe lapply() allows you to perform complex tasks.\n\n## Column names ----\ncolumns &lt;- c(\"bill_length_mm\", \"bill_depth_mm\", \"body_mass_g\")\n\n## Mean, min and max of columns 3, 4 and 5 ----\nvalues &lt;- lapply(columns, function(x) {\n  column &lt;- penguins[ , x, drop = TRUE]\n  data.frame(\"trait\" = x,\n             \"mean\"  = mean(column, na.rm = TRUE),\n             \"min\"   = min(column, na.rm = TRUE),\n             \"max\"   = max(column, na.rm = TRUE))\n})\n\nvalues\n\n[[1]]\n           trait     mean  min  max\n1 bill_length_mm 43.92193 32.1 59.6\n\n[[2]]\n          trait     mean  min  max\n1 bill_depth_mm 17.15117 13.1 21.5\n\n[[3]]\n        trait     mean  min  max\n1 body_mass_g 4201.754 2700 6300\n\n\nLet’s simplify the output into a single data.frame by recursively applying (with do.call()) the function rbind.data.frame() to each data.frame of the list.\n\n## Simplify output ----\nvalues &lt;- do.call(rbind.data.frame, values)\n\nvalues\n\n           trait       mean    min    max\n1 bill_length_mm   43.92193   32.1   59.6\n2  bill_depth_mm   17.15117   13.1   21.5\n3    body_mass_g 4201.75439 2700.0 6300.0\n\n\nNB. Here the object penguins is retrieved from the global environment. But it’s safer to explicitly use it like this:\n\n## Column names ----\ncolumns &lt;- c(\"bill_length_mm\", \"bill_depth_mm\", \"body_mass_g\")\n\n## Mean, min and max of columns 3, 4 and 5 ----\nvalues &lt;- lapply(columns, function(x, data) {\n  column &lt;- data[ , x, drop = TRUE]\n  data.frame(\"trait\" = x,\n             \"mean\"  = mean(column, na.rm = TRUE),\n             \"min\"   = min(column, na.rm = TRUE),\n             \"max\"   = max(column, na.rm = TRUE))\n}, data = penguins)\n\ndo.call(rbind.data.frame, values)\n\n           trait       mean    min    max\n1 bill_length_mm   43.92193   32.1   59.6\n2  bill_depth_mm   17.15117   13.1   21.5\n3    body_mass_g 4201.75439 2700.0 6300.0"
  },
  {
    "objectID": "posts/2024-11-19-the-apply-function-family/index.html#the-tapply-function",
    "href": "posts/2024-11-19-the-apply-function-family/index.html#the-tapply-function",
    "title": "The apply() function family",
    "section": "The tapply() function",
    "text": "The tapply() function\nThe tapply() allows you to perform a function across specified groups in your data. For dplyr users, it’s equivalent to the group_by() and summarize() functions.\n\nthe first argument X specifies the values\nthe second argument INDEX specifies the groups\nthe third argument FUN is the function to apply\n\nLets’ compute the mean of bill_length_mm for each species.\n\n## Average bill length for each species ----\ntapply(penguins$\"bill_length_mm\", penguins$\"species\", function(x) {\n  mean(x, na.rm = TRUE)\n})\n\n   Adelie Chinstrap    Gentoo \n 38.79139  48.83382  47.50488 \n\n\nWe can group values according to two variables.\n\n## Average bill length for each species ----\ntapply(penguins$\"bill_length_mm\", \n       list(penguins$\"species\", penguins$\"island\"), \n       function(x) mean(x, na.rm = TRUE))\n\n            Biscoe    Dream Torgersen\nAdelie    38.97500 38.50179  38.95098\nChinstrap       NA 48.83382        NA\nGentoo    47.50488       NA        NA\n\n\nHere the output is a matrix. We can convert it to long data.frame w/ tidyr::pivot_longer().\n\n## Load 'dplyr' package ----\nlibrary(\"tidyr\")\n\n## Average bill length for each species and island ----\nvalues &lt;- tapply(penguins$\"bill_length_mm\", \n                 list(penguins$\"species\", penguins$\"island\"), \n                 function(x) mean(x, na.rm = TRUE))\n\n## Convert to data.frame ----\nvalues &lt;- data.frame(values)\nvalues$\"species\" &lt;- rownames(values)\n\n\n## Pivot data ----  \nvalues |&gt; \n  pivot_longer(cols      = !species,\n               values_to = \"bill_length_mm\",\n               names_to  = \"island\")\n\n# A tibble: 9 × 3\n  species   island    bill_length_mm\n  &lt;chr&gt;     &lt;chr&gt;              &lt;dbl&gt;\n1 Adelie    Biscoe              39.0\n2 Adelie    Dream               38.5\n3 Adelie    Torgersen           39.0\n4 Chinstrap Biscoe              NA  \n5 Chinstrap Dream               48.8\n6 Chinstrap Torgersen           NA  \n7 Gentoo    Biscoe              47.5\n8 Gentoo    Dream               NA  \n9 Gentoo    Torgersen           NA  \n\n\nThis is equivalent to dplyr approach.\n\n## Load 'dplyr' package ----\nlibrary(\"dplyr\")\n\n## Summarise data ----\npenguins %&gt;%\n  group_by(species, island) %&gt;%\n  summarize(bill_length_mm = mean(bill_length_mm, \n                                  na.rm = TRUE)) %&gt;%\n  ungroup()\n\n# A tibble: 5 × 3\n  species   island    bill_length_mm\n  &lt;fct&gt;     &lt;fct&gt;              &lt;dbl&gt;\n1 Adelie    Biscoe              39.0\n2 Adelie    Dream               38.5\n3 Adelie    Torgersen           39.0\n4 Chinstrap Dream               48.8\n5 Gentoo    Biscoe              47.5"
  },
  {
    "objectID": "posts/2024-06-11-python-tutorial-part-2/index.html",
    "href": "posts/2024-06-11-python-tutorial-part-2/index.html",
    "title": "Python tutorial - Part 2",
    "section": "",
    "text": "This post has a dedicated presentation available here.\nThe source of the Quarto presentation is available on GitHub."
  },
  {
    "objectID": "posts/2026-01-06-occupancy-models-in-r/index.html",
    "href": "posts/2026-01-06-occupancy-models-in-r/index.html",
    "title": "Occupancy models in R",
    "section": "",
    "text": "Question. Why do we talk out loud when we know we’re alone? Conjecture. Because we know we’re not.\n– The Twelfth Doctor, Doctor Who (Series 8, Episode 4: “Listen”)"
  },
  {
    "objectID": "posts/2026-01-06-occupancy-models-in-r/index.html#introduction",
    "href": "posts/2026-01-06-occupancy-models-in-r/index.html#introduction",
    "title": "Occupancy models in R",
    "section": "Introduction",
    "text": "Introduction\nIt’s not because we didn’t see something that this thing wasn’t present. At least, that’s the idea behind occupancy models: our observations aren’t a direct representation of reality, but merely of what we can detect.\n\n\n\nPresent (\\(z_i = 1\\)), but not detected (\\(y_{ij} = 0\\)). Photo by Caroline Kirk (source)\n\n\nOccupancy models were first published by MacKenzie et al. (2002) in the context of species occurrence modelling. Many extensions of occupancy have been proposed since, allowing to explicitly model occupancy dynamics (MacKenzie et al. 2003), take into account multiple species (Rota et al. 2016) or a continuous detection process (MacKenzie et al. 2003). This blog post only goes over the original simple occupancy model."
  },
  {
    "objectID": "posts/2026-01-06-occupancy-models-in-r/index.html#simple-occupancy-model",
    "href": "posts/2026-01-06-occupancy-models-in-r/index.html#simple-occupancy-model",
    "title": "Occupancy models in R",
    "section": "Simple occupancy model",
    "text": "Simple occupancy model\n\n\n\nSummary diagram of the structure of an occupancy model.\n\n\nTo discriminate between the real and the observed states, occupancy models have one parameter for each of these states. The true presence or absence of a species in a given site \\(i\\) is noted \\(z_i\\). The observed presence or absence at site \\(i\\) for a given visit \\(j\\) is noted \\(y_{ij}\\).\nThen, the occupancy model for a given site \\(i\\) is written as:\n\\[\ny_{ij} \\sim Bern(z_i~p)\n\\]\n\nif the species is really present in site \\(i\\) (\\(z_i = 1\\)), then it is detected (\\(y_{ij} = 1\\)) with probability \\(p\\) according to a Bernoulli trial.\nif the species is absent in site \\(i\\) (\\(z_i = 0\\)), then we assume that it cannot be detected (\\(y_{ij}\\) has to be zero) (no false detection).\n\nIn the occupancy framework, \\(z_i\\) itself is governed by a Bernoulli trial of probability \\(\\psi\\).\n\\[\nz_i \\sim Bern(\\psi)\n\\]\n\\(\\psi\\) is called the occupancy probability: most of the times, when dealing with occupancy, that’s the quantity we’re really interested in."
  },
  {
    "objectID": "posts/2026-01-06-occupancy-models-in-r/index.html#simulate-occupancy",
    "href": "posts/2026-01-06-occupancy-models-in-r/index.html#simulate-occupancy",
    "title": "Occupancy models in R",
    "section": "Simulate occupancy",
    "text": "Simulate occupancy\nOccupancy models are really easy to simulate: here is a sample R code to simulate data under a simple occupancy model.\n\nset.seed(42)\n\nM &lt;- 100 # Number of sites\np &lt;- 0.4 # Detection probability\npsi &lt;- 0.8 # Occupancy\n\n# Simulate a number of visits for each site\nnvisit &lt;- rpois(n = M, lambda = 3)\nnvisit[nvisit == 0] &lt;- 1 # Don't allow zero visits\n\n# Initialize vectors\nz &lt;- vector(mode = \"numeric\", length = M)\ny &lt;- vector(mode = \"list\", length = M)\n\nfor (i in 1:M) { # For each site\n  # Simulate true presence/absence at site i\n  zi &lt;- rbinom(n = 1, size = 1, prob = psi)\n  \n  # Simulate observed presence/absence at site i for all visits\n  yij &lt;- rbinom(n = nvisit[i], \n                size = 1, prob = p*zi)\n  \n  z[i] &lt;- zi # True sites states\n  y[[i]] &lt;- yij # Detections\n}\n\nIn this simulation, the true proportion of occupied sites is 0.73. It is very close to the occupancy parameter \\(\\psi = 0.8\\) (but it is not exactly equal because of the stochastic nature of our model).\n\n# True proportion of occupied sites\nsum(z)/M\n\n[1] 0.73\n\n\nThe proportion of sites that we detect as occupied (what is called the naive occupancy) is 0.51, which wildly under-estimates \\(\\psi\\).\n\n# Proportion of sites with at least one detection\nsum(sapply(y, function(yi) any(yi != 0)))/M\n\n[1] 0.51"
  },
  {
    "objectID": "posts/2026-01-06-occupancy-models-in-r/index.html#infer-occupancy-models-in-r",
    "href": "posts/2026-01-06-occupancy-models-in-r/index.html#infer-occupancy-models-in-r",
    "title": "Occupancy models in R",
    "section": "Infer occupancy models in R",
    "text": "Infer occupancy models in R\nNow, the true value of occupancy models lies in the analysis of species detections. Inferring parameters from data is possible under three main conditions:\n\nYou have repeated visits. This is an crucial point which allows parameter identifiability.\nThe site remains in the same state (occupied or unoccupied) during the entire study period (closure assumption)1.\nThere are no false detections. The model automatically assumes that a detection event means that the site is really occupied, and false detections might induce a positive bias in occupancy estimates.\n\nThere are lots of strategies to infer occupancy models in R: among the most popular occupancy packages are unmarked and spOccupancy. Many people will also use Bayesian softwares like JAGS (e.g. with the R package jagsUI or rjags), nimble or Stan (with the R packagescmdstanrorrstan).\nThis blog post focuses on two of them: unmarked and Stan (using the R interface package cmdstanr).\n\nunmarked\nThe R package unmarked has functions specifically designed for occupancy inference in R using maximum likelihood estimation (frequentist statistics).\nFirst, we have to format the observed visits to an unmarkedFrameOccu object, as exemplified below:\n\nlibrary(unmarked)\n\n# Format the list of observed detections y\nmax_visit &lt;- max(sapply(y, length)) # Get maximum number of detections\n\n# Transform y to matrix\ny_matrix &lt;- matrix(data = NA,\n                   nrow = M,\n                   ncol = max_visit)\nfor (i in 1:M) { # For each site\n  nvisit_i &lt;- length(y[[i]]) # Get number of visits\n  y_matrix[i, 1:nvisit_i] &lt;- y[[i]] # Fill n-th first rows with detection history\n}\n\n# Each row contains the detections at a site, filled with NAs for \n# sites that have less visits than the most visited one\nhead(y_matrix, 3)\n\n     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8]\n[1,]    0    0    0    1    1   NA   NA   NA\n[2,]    1    0    0    1    1    1   NA   NA\n[3,]    0    0   NA   NA   NA   NA   NA   NA\n\n# Cast y_matrix to unmarkedFrameOccu\ny_occu &lt;- unmarked::unmarkedFrameOccu(y_matrix)\nhead(y_occu, 3)\n\nData frame representation of unmarkedFrame object.\n  y.1 y.2 y.3 y.4 y.5 y.6 y.7 y.8\n1   0   0   0   1   1  NA  NA  NA\n2   1   0   0   1   1   1  NA  NA\n3   0   0  NA  NA  NA  NA  NA  NA\n\n\nIn unmarked, the main function to infer parameters of a simpler occupancy model is occu, which uses a maximum likelihood estimator. In its simplest form, it only needs two arguments:\n\nformula, which gives the formulas for the logit of \\(p\\) and \\(\\psi\\). Here, since occupancy and detection are constant, we will set them to ~1 ~1.\ndata: an unmarkedFrameOccu object containing observed detections (here, y_occu).\n\n\n# Infer occupancy\nocc &lt;- unmarked::occu(formula = ~1 ~1, \n                      data = y_occu)\n\nclass(occ)\n\n[1] \"unmarkedFitOccu\"\nattr(,\"package\")\n[1] \"unmarked\"\n\nsummary(occ)\n\n\nCall:\nunmarked::occu(formula = ~1 ~ 1, data = y_occu)\n\nOccupancy (logit-scale):\n Estimate    SE    z P(&gt;|z|)\n     1.03 0.406 2.54  0.0112\n\nDetection (logit-scale):\n Estimate    SE     z  P(&gt;|z|)\n   -0.623 0.184 -3.38 0.000722\n\nAIC: 360.8983 \nNumber of sites: 100\n\n\nThe output of the model is an unmarkedFitOccu object. The summary gives us the parameter estimates on the logit scale (i.e. we get \\(\\text{logit}(p)\\) and \\(\\text{logit}(\\psi)\\)), but we can get estimates on the natural scale with backTransform:\n\n# Get detection (p) on the natural scale\nunmarked::backTransform(occ, type = \"det\")\n\nBacktransformed linear combination(s) of Detection estimate(s)\n\n Estimate     SE LinComb (Intercept)\n    0.349 0.0419  -0.623           1\n\nTransformation: logistic \n\n# Get occupancy (state parameter, psi) on the natural scale\nunmarked::backTransform(occ, type = \"state\")\n\nBacktransformed linear combination(s) of Occupancy estimate(s)\n\n Estimate     SE LinComb (Intercept)\n    0.737 0.0788    1.03           1\n\nTransformation: logistic \n\n\nAnd inferred parameters are a good approximation of true values (see figure below).\n\n\nCode\nlibrary(ggplot2)\n\np_inf &lt;- unmarked::predict(occ, \n                           type = \"det\",\n                           backTransform = TRUE,\n                           newdata = data.frame(1))\npsi_inf &lt;- unmarked::predict(occ, \n                             type = \"state\",\n                             backTransform = TRUE,\n                             newdata = data.frame(1))\n\numk_param_df &lt;- data.frame(param = c(\"p\", \"p\", \"psi\", \"psi\"),\n                           type = c(\"inferred\", \"true\", \"inferred\", \"true\"),\n                           estimate = c(p_inf$Predicted, p,\n                                        psi_inf$Predicted, psi),\n                           min = c(p_inf$lower, NA,\n                                   psi_inf$lower, NA),\n                           max = c(p_inf$upper, NA,\n                                   psi_inf$upper, NA))\n\nggplot(umk_param_df) +\n  geom_errorbar(data = subset(umk_param_df, type == \"inferred\"),\n                aes(xmin = min, xmax = max, y = param,\n                    color = type)) +\n  geom_point(aes(x = estimate, y = param,\n                 color = type)) +\n  theme_bw(base_size = 15) +\n  xlim(0, 1) +\n  theme(axis.title = element_blank()) +\n  ggtitle(\"True and inferred occupancy parameters with unmarked\")\n\n\n\n\n\n\n\n\n\n\n\nStan\nBayesian frameworks are also quite common to infer occupancy models, notably because they allow for more flexibility in the parameters specifications. Here, we use Stan through the R package cmdstanr. Stan is a Bayesian software using optimized algorithms for the MCMC samplers.\nIn Stan, it is more efficient to use a Binomial model specification: we model the number of detections \\(n_i\\) at each site:\n\\[n_i \\sim Binom({n_{\\text{visit}}}_i, z_i~p) \\quad \\text{and} \\quad z_i \\sim Bern(\\psi)\\] where \\({n_{\\text{visit}}}_i\\) the number of visits at site \\(i\\). The equations above are strictly equivalent to the general occupancy model specification described earlier.\nThe first step is to format our data to be used by Stan. For that, we aggregate our visits to total number of detections, and save data parameters to a list named dat.\n\n# Format data for Stan\nn &lt;- sapply(y, sum) # Number of detections\nnvisit &lt;- sapply(y, length) # Number of visits\n\n# List of parameters for Stan\ndat &lt;- list(M = M,\n            n = n,\n            nvisit = nvisit)\n\n\nStan code\nThen, we write the Stan code to infer our occupancy parameters. Stan is a programming language in its own right, and here we will only go over the Stan syntax quickly.\nStan programs are organized in code blocks, which are indicated by opening and closing brackets as shown below:\ndata {\n  // Code block to define data variables\n}\n\nparameters {\n  // Code block to define parameters\n}\n\nmodel {\n  // Code block to infer parameters\n}\nNote that Stan comments are specified with a double backslash //. There are other optional code blocks (in fact, we will use one later), but the mandatory ones are shown above.\nLet’s start by defining variables in the data block. Stan variables are typed (i.e. we must define their type manually with the statements before variables names). Here, we only need the 3 variables that we specified on our data list above.\ndata {\n  int&lt;lower=1&gt; M; // Number of sites\n  array[M] int nvisit; // Number of visits per sites-years\n  array[M] int&lt;lower=0&gt; n; // Observations vector\n}\nNext, we define the parameters we want to infer in the parameters block. Here, they are defined on the logit scale because inferring unbounded parameters is more efficient in Stan.\nparameters {\n  real psi_logit; // Value of psi on the logit scale\n  real p_logit; // Value of p on the logit scale\n}\nThe syntax of the model block is slightly more complex. There are 3 steps:\n\nDefine the parameters priors: here, we use flat normal priors centered on zero with a standard deviation of 3. This amounts to initializing the log-posterior with a log-transformed normal density, which is what target += normal_lpdf(param | 0, 3) does.\n(Optional) Define intermediate variables. Here, we define nvi as a shortcut for the number of visits for site \\(i\\).\nSpecify the model. This is where it requires a bit of work, because Stan cannot model discrete variables, so we have to update the posterior probability density manually instead. Fortunately, it is fairly easy to write with respect to \\(p\\) and \\(\\psi\\). Here, we won’t go into the details of these computations (but see Note 1 below).\n\nmodel {\n  // 1. Priors\n  target += normal_lpdf(psi_logit | 0, 3);\n  target += normal_lpdf(p_logit | 0, 3);\n\n  // 2. Variables\n  int nvi;\n\n  // 3. Model specification\n  for (i in 1:M) { // Iterate over sites\n    nvi = nvisit[i]; // Number of visits\n    if (n[i] &gt; 0) { // The species was seen\n      // Update log-likelihood: species was detected | present\n      target += log_inv_logit(psi_logit) + \n        binomial_logit_lpmf(n[i] | nvi, p_logit);\n    } else {\n      // Update log-likelihood: species was non-detected | present\n      // or non-detected | absent\n      target += log_sum_exp(log_inv_logit(psi_logit) + binomial_logit_lpmf(0 | nvi, p_logit), \n        log1m_inv_logit(psi_logit));\n    }\n  }\n}\n\n\n\n\n\n\nNote 1: Log-posterior probability density computation\n\n\n\nBelow, we give a bit more explanations about the computation of the log-posterior.\nWe distinguish between two cases to write the log-posterior:\n\nThe species was detected at least once (case n[i] &gt; 0). In that case, we know that it was present (no false detections), so the conditional log-probability that the species was seen \\(n_i\\) times (which is what we update target with) is the product of the occupancy probability \\(\\psi\\) and the binomial density probability of \\(N = n\\): \\[P(N = n_i | \\{p, \\psi\\}) = \\psi ~ \\underbrace{\\binom{{n_{\\text{visit}}}_i}{n_i} p^{n_i} p^{{n_{\\text{visit}}}_i - n_i}}_{\\text{Binomial density probability}}\\] In Stan, parameters are on the logit scale: psi_logit is first back-transformed with log_inv_logit and Stan uses the Binomial density probability on the logit scale with binomial_logit_lpmf. Finally, because we compute the log-posterior, by properties of the logarithm the multiplication is changed to an addition.\nWhen the species was not seen (else block), we have two options. Either the species was present, but undetected; or it was absent. In that case, the conditional log-probability is the sum of these two events: \\[P(N = 0 | \\{p, \\psi\\}) = \\underbrace{\\psi ~ \\binom{{n_{\\text{visit}}}_i}{0} p^{0} p^{{n_{\\text{visit}}}_i - 0}}_{\\text{present, undetected}} + \\underbrace{(1 - \\psi)}_{\\text{absent}}\\] In Stan, log_sum_exp is an efficient way to compute a logarithm of the sum above, and log1m_inv_logit(psi_logit) returns \\(1 - \\psi\\).\n\nMore details on log-likelihood computation can be found in this blog post or in chapter 2.4.6 of Kéry and Royle (2016).\n\n\nThe final (and optional) code block we use here is a generated quantities block. It allows to compute \\(p\\) and \\(\\psi\\) on the natural scale by using inv_logit to back-transform parameters.\ngenerated quantities {\n  real&lt;lower=0,upper=1&gt; p = inv_logit(p_logit);\n  real&lt;lower=0, upper=1&gt; psi = inv_logit(psi_logit);\n}\nThe final Stan code is summarised below:\ndata {\n  int&lt;lower=1&gt; M; // Number of sites\n  array[M] int nvisit; // Number of visits per sites-years\n  array[M] int&lt;lower=0&gt; n; // Observations vector\n}\n\nparameters {\n  real psi_logit; // Value of psi on the logit scale\n  real p_logit; // Value of p on the logit scale\n}\n\nmodel {\n  // 1. Priors\n  target += normal_lpdf(psi_logit | 0, 3);\n  target += normal_lpdf(p_logit | 0, 3);\n\n  // 2. Variables\n  int nvi;\n\n  // 3. Model specification\n  for (i in 1:M) { // Iterate over sites\n    nvi = nvisit[i]; // Number of visits\n    if (n[i] &gt; 0) { // The species was seen\n      // Update log-likelihood: species was detected | present\n      target += log_inv_logit(psi_logit) + binomial_logit_lpmf(n[i] | nvi, p_logit);\n    } else {\n      // Update log-likelihood: species was non-detected | present\n      // or non-detected | absent\n      target += log_sum_exp(log_inv_logit(psi_logit) + binomial_logit_lpmf(0 | nvi, p_logit), log1m_inv_logit(psi_logit));\n    }\n  }\n}\n\ngenerated quantities {\n  real&lt;lower=0,upper=1&gt; p = inv_logit(p_logit);\n  real&lt;lower=0, upper=1&gt; psi = inv_logit(psi_logit);\n}\n\n\nInference with cmdstanr\nNow, we can use this code to infer our occupancy parameters from R. The first step is to compile the Stan model using cmdstan_model, as shown in the code below. Here, we assume the Stan code above is written in a file, which path is stored in the stan_file variable in R.\n\nlibrary(cmdstanr)\n\nmodel &lt;- cmdstanr::cmdstan_model(stan_file = stan_file)\n\nWe can finally perform the inference on our model object, with the $sample method from the CmdStanModel class. The $sample method can be customised with many arguments, but here we only use our data list (data), the number of burn-in iterations (warmup), the number of post-burn-in iterations (iter_sampling) and the number of chains and their parallelisation (chains and parallel_chains).\n\nstanfit &lt;- model$sample(data = dat,\n                        iter_warmup = 200,\n                        iter_sampling = 500,\n                        chains = 4,\n                        parallel_chains = 4)\n\nWe can inspect the inferred values with the $summary method.\n\n(stan_inf &lt;- stanfit$summary())\n\n# A tibble: 5 × 10\n  variable      mean   median     sd    mad       q5      q95  rhat ess_bulk\n  &lt;chr&gt;        &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n1 lp__      -115.    -114.    1.02   0.801  -117.    -114.     1.01     882.\n2 psi_logit    1.18     1.11  0.484  0.440     0.491    2.06   1.00    1011.\n3 p_logit     -0.651   -0.649 0.190  0.183    -0.970   -0.332  1.00     771.\n4 p            0.344    0.343 0.0427 0.0409    0.275    0.418  1.00     771.\n5 psi          0.754    0.753 0.0795 0.0808    0.620    0.887  1.00    1011.\n# ℹ 1 more variable: ess_tail &lt;dbl&gt;\n\n\nThe summary gives information about the log-posterior (lp__) and all inferred parameters (on the logit and the natural scales). For each parameter, a set of summary statistics which summarize their distribution are given. Three diagnostic parameters to evaluate MCMC sampling are also given (a convergence statistic \\(\\hat{R}\\), and effective sample sizes (ESS)).\nFinally, a graphical summary shows that inferred parameters are close to the real ones:\n\n\nCode\nlibrary(ggplot2)\n\nstan_param_df &lt;- data.frame(param = c(\"p\", \"p\", \"psi\", \"psi\"),\n                            type = c(\"inferred\", \"true\", \"inferred\", \"true\"),\n                            estimate = c(stan_inf$mean[stan_inf$variable == \"p\"], p,\n                                         stan_inf$mean[stan_inf$variable == \"psi\"], psi),\n                            min = c(stan_inf$q5[stan_inf$variable == \"p\"], NA,\n                                    stan_inf$q5[stan_inf$variable == \"psi\"], NA),\n                            max = c(stan_inf$q95[stan_inf$variable == \"p\"], NA,\n                                    stan_inf$q95[stan_inf$variable == \"psi\"], NA))\n\nggplot(stan_param_df) +\n  geom_errorbar(data = subset(stan_param_df, type == \"inferred\"),\n                aes(xmin = min, xmax = max, y = param,\n                    color = type)) +\n  geom_point(aes(x = estimate, y = param,\n                 color = type)) +\n  theme_bw(base_size = 15) +\n  xlim(0, 1) +\n  theme(axis.title = element_blank()) +\n  ggtitle(\"True and inferred occupancy parameters with Stan\")"
  },
  {
    "objectID": "posts/2026-01-06-occupancy-models-in-r/index.html#conclusion",
    "href": "posts/2026-01-06-occupancy-models-in-r/index.html#conclusion",
    "title": "Occupancy models in R",
    "section": "Conclusion",
    "text": "Conclusion\nIn this blog post, we have seen how the simple occupancy model is written and how simulate data under an occupancy model in R. We have then inferred occupancy models with the unmarked and cmdstanr R packages.\nThere is much more one can do with occupancy models. In particular, the parameters \\(p\\) and \\(\\psi\\) are often not constant, and can be modeled as functions of covariates with a logit link, such as \\(\\text{logit}(\\psi_i) = \\beta_0 + \\beta_1 x_i\\) or \\(\\text{logit}(p_{ij}) = \\beta_0 + \\beta_1 x_{ij}\\). But this is a story for another blog post…"
  },
  {
    "objectID": "posts/2026-01-06-occupancy-models-in-r/index.html#references",
    "href": "posts/2026-01-06-occupancy-models-in-r/index.html#references",
    "title": "Occupancy models in R",
    "section": "References",
    "text": "References\n\n\nKéry, Marc, and J. Andrew Royle. 2016. Applied Hierarchical Modeling in Ecology: Analysis of Distribution, Abundance and Species Richness in R and BUGS. Amsterdam; Boston: Elsevier/AP, Academic Press is an imprint of Elsevier.\n\n\nMacKenzie, Darryl I., James D. Nichols, James E. Hines, Melinda G. Knutson, and Alan B. Franklin. 2003. “Estimating Site Occupancy, Colonization, and Local Extinction When a Species Is Detected Imperfectly.” Ecology 84 (8): 2200–2207. https://doi.org/10.1890/02-3090.\n\n\nMacKenzie, Darryl I., James D. Nichols, Gideon B. Lachman, Sam Droege, J. Andrew Royle, and Catherine A. Langtimm. 2002. “Estimating Site Occupancy Rates When Detection Probabilities Are Less Than One.” Ecology 83 (8): 2248–55. https://doi.org/10.1890/0012-9658(2002)083[2248:ESORWD]2.0.CO;2.\n\n\nRota, Christopher T., Marco A. R. Ferreira, Roland W. Kays, Tavis D. Forrester, Elizabeth L. Kalies, William J. McShea, Arielle W. Parsons, and Joshua J. Millspaugh. 2016. “A Multispecies Occupancy Model for Two or More Interacting Species.” Methods in Ecology and Evolution 7 (10): 1164–73. https://onlinelibrary.wiley.com/doi/abs/10.1111/2041-210X.12587.\n\n\nValente, Jonathon J., Vitek Jirinec, and Matthias Leu. 2024. “Thinking Beyond the Closure Assumption: Designing Surveys for Estimating Biological Truth with Occupancy Models.” Methods in Ecology and Evolution 15 (12): 2289–2300. https://doi.org/10.1111/2041-210X.14439."
  },
  {
    "objectID": "posts/2026-01-06-occupancy-models-in-r/index.html#other-resources",
    "href": "posts/2026-01-06-occupancy-models-in-r/index.html#other-resources",
    "title": "Occupancy models in R",
    "section": "Other resources",
    "text": "Other resources\n\nRichard A. Erickson’s GitHub repository with Stan occupancy models tutorials\nOlivier Gimenez’s workshop with resources on occupancy modelling with unmarked"
  },
  {
    "objectID": "posts/2026-01-06-occupancy-models-in-r/index.html#footnotes",
    "href": "posts/2026-01-06-occupancy-models-in-r/index.html#footnotes",
    "title": "Occupancy models in R",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIn practice, this assumption is often violated in studies using occupancy models: this has been discussed extensively in the literature (see for example Valente, Jirinec, and Leu (2024)), and authors have proposed to critically evaluate whether the closure assumption is met to interpret the inferred occupancy.↩︎"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Tips and tricks",
    "section": "",
    "text": "This blog lists tips and tricks shared during the FRB-CESAB Geek meetings. We love to talk about R, RStudio, git, GitHub, OS, Website, etc. and we are delighted to share our experience with the community."
  },
  {
    "objectID": "index.html#welcome",
    "href": "index.html#welcome",
    "title": "Tips and tricks",
    "section": "",
    "text": "This blog lists tips and tricks shared during the FRB-CESAB Geek meetings. We love to talk about R, RStudio, git, GitHub, OS, Website, etc. and we are delighted to share our experience with the community."
  },
  {
    "objectID": "CONTRIBUTING.html",
    "href": "CONTRIBUTING.html",
    "title": "Contributing",
    "section": "",
    "text": "First off, thanks for taking the time to contribute to tips-and-tricks!\nAll types of contributions are encouraged and valued. See the Table of contents for different ways to help and details about how this project handles them. Please make sure to read the relevant section before making your contribution. It will make it a lot easier for us maintainers and smooth out the experience for all involved.\n\n\n\nCode of conduct\nStyle guide\nCommit messages\nAsking questions\nReporting bugs\nRequesting features\nContributing code\n\n\n\n\nThis project is released with a Contributor Code of Conduct. By participating, you are expected to uphold this code. Please report unacceptable behavior to nicolas.casajus@fondationbiodiversite.fr.\n\n\n\nWe use the Tidyverse style guide for writing R code. Functions are documented with the roxygen2 syntax. tips-and-tricks uses the lower_snake_case.\n\n\n\nIf you want to contribute by commiting changes, please try to use the Conventional commits specification.\n\n\n\nBefore you ask a question, it is best to search for existing Issues that might help you. In case you have found a suitable issue and still need clarification, you can write your question in this issue.\nIf you then still feel the need to ask a question and need clarification, we recommend the following:\n\nOpen a new Issue.\nUse the template other_issue.md.\nProvide as much context as you can about what you’re running into.\nProvide project and platform versions if relevant (paste the output of sessionInfo()).\n\nWe will then take care of the issue as soon as possible.\n\n\n\n\n\nA good bug report shouldn’t leave others needing to chase you up for more information. Therefore, we ask you to investigate carefully, collect information and describe the issue in detail in your report. Please complete the following steps in advance to help us fix any potential bug as fast as possible.\n\nMake sure that you are using the latest version of tips-and-tricks.\nDetermine if your bug is really a bug and not an error on your side.\nTo see if other users have experienced (and potentially already solved) the same issue you are having, check if there is not already a bug report existing for your bug or error in the bug tracker.\n\n\n\n\nWe use GitHub Issues to track bugs and errors. If you run into an issue with the project:\n\nOpen a new Issue.\nUse the template bug_report.md.\nExplain the behavior you would expect and the actual behavior.\nPlease provide as much context as possible and describe the reproduction steps that someone else can follow to recreate the issue on their own. This usually includes your code with a reproducible example.\n\nWe will then take care of the issue as soon as possible.\n\n\n\n\n\n\n\nMake sure that you are using the latest version of tips-and-tricks.\nRead the documentation carefully and find out if the functionality is already covered.\nPerform a search to see if this enhancement has already been suggested. If it has, add a comment to the existing issue instead of opening a new one.\n\n\n\n\nFeature requests are tracked as GitHub Issues.\n\nOpen a new Issue.\nUse the template feature_request.md.\nProvide a clear and descriptive title for the issue to identify the suggestion.\nProvide a step-by-step description of the suggested enhancement in as many details as possible.\nExplain why this enhancement would be useful to most tips-and-tricks users.\n\nWe will then take care of the issue as soon as possible.\n\n\n\n\n\n\nWe use the GitHub flow to collaborate on this project:\n\nFork this repository using the GitHub interface.\nClone your fork using git clone fork-url (replace fork-url by the URL of your fork). Alternatively, open RStudio IDE and create a New Project from Version Control.\nCreate a new branch w/ git checkout -b branch-name (replace branch-name by the name of your new branch).\nMake your contribution (see below for examples).\nStage (git add) and commit (git commit) your changes as often as necessary\nPush your changes to GitHub w/ git push origin branch-name.\nSubmit a Pull Request on the original repo.\n\nWe will then review the PR as soon as possible.\n\n\n\n\n\nIf you want to contribute by improving the README, please edit the README.Rmd (not the README.md). Do not forget to update the README.md by running:\nrmarkdown::render(\"README.Rmd\")\n\n\n\nIf you want to contribute by improving the documentation of a function, open the corresponding file in the R/ folder and edit lines starting with #' (roxygen2 syntax).\nUpdate the documentation (Rd files in the man/ folder) by running:\ndevtools::document()\nIf you use a new external dependency in the example section, do not forget to add it in the DESCRIPTION file under the section Imports (only if this package is not already listed).\nCheck the integrity of the package with:\ndevtools::check()\n\n\n\n\nIf you want to contribute by improving the code of a function, open and edit the corresponding file in the R/ folder.\nCheck the integrity of the package with:\ndevtools::check()\nDo not forget to adapt the unit tests for the function by editing the corresponding file stored in the tests/testthat/ folder. We use the package testthat to implement unit tests.\nCheck your tests by running:\ndevtools::test()\n\n\n\nIf you want to contribute by writing a new post, please read this page.\nThanks for your contribution!"
  },
  {
    "objectID": "CONTRIBUTING.html#table-of-contents",
    "href": "CONTRIBUTING.html#table-of-contents",
    "title": "Contributing",
    "section": "",
    "text": "Code of conduct\nStyle guide\nCommit messages\nAsking questions\nReporting bugs\nRequesting features\nContributing code"
  },
  {
    "objectID": "CONTRIBUTING.html#code-of-conduct",
    "href": "CONTRIBUTING.html#code-of-conduct",
    "title": "Contributing",
    "section": "",
    "text": "This project is released with a Contributor Code of Conduct. By participating, you are expected to uphold this code. Please report unacceptable behavior to nicolas.casajus@fondationbiodiversite.fr."
  },
  {
    "objectID": "CONTRIBUTING.html#style-guide",
    "href": "CONTRIBUTING.html#style-guide",
    "title": "Contributing",
    "section": "",
    "text": "We use the Tidyverse style guide for writing R code. Functions are documented with the roxygen2 syntax. tips-and-tricks uses the lower_snake_case."
  },
  {
    "objectID": "CONTRIBUTING.html#commit-messages",
    "href": "CONTRIBUTING.html#commit-messages",
    "title": "Contributing",
    "section": "",
    "text": "If you want to contribute by commiting changes, please try to use the Conventional commits specification."
  },
  {
    "objectID": "CONTRIBUTING.html#asking-questions",
    "href": "CONTRIBUTING.html#asking-questions",
    "title": "Contributing",
    "section": "",
    "text": "Before you ask a question, it is best to search for existing Issues that might help you. In case you have found a suitable issue and still need clarification, you can write your question in this issue.\nIf you then still feel the need to ask a question and need clarification, we recommend the following:\n\nOpen a new Issue.\nUse the template other_issue.md.\nProvide as much context as you can about what you’re running into.\nProvide project and platform versions if relevant (paste the output of sessionInfo()).\n\nWe will then take care of the issue as soon as possible."
  },
  {
    "objectID": "CONTRIBUTING.html#reporting-bugs",
    "href": "CONTRIBUTING.html#reporting-bugs",
    "title": "Contributing",
    "section": "",
    "text": "A good bug report shouldn’t leave others needing to chase you up for more information. Therefore, we ask you to investigate carefully, collect information and describe the issue in detail in your report. Please complete the following steps in advance to help us fix any potential bug as fast as possible.\n\nMake sure that you are using the latest version of tips-and-tricks.\nDetermine if your bug is really a bug and not an error on your side.\nTo see if other users have experienced (and potentially already solved) the same issue you are having, check if there is not already a bug report existing for your bug or error in the bug tracker.\n\n\n\n\nWe use GitHub Issues to track bugs and errors. If you run into an issue with the project:\n\nOpen a new Issue.\nUse the template bug_report.md.\nExplain the behavior you would expect and the actual behavior.\nPlease provide as much context as possible and describe the reproduction steps that someone else can follow to recreate the issue on their own. This usually includes your code with a reproducible example.\n\nWe will then take care of the issue as soon as possible."
  },
  {
    "objectID": "CONTRIBUTING.html#requesting-features",
    "href": "CONTRIBUTING.html#requesting-features",
    "title": "Contributing",
    "section": "",
    "text": "Make sure that you are using the latest version of tips-and-tricks.\nRead the documentation carefully and find out if the functionality is already covered.\nPerform a search to see if this enhancement has already been suggested. If it has, add a comment to the existing issue instead of opening a new one.\n\n\n\n\nFeature requests are tracked as GitHub Issues.\n\nOpen a new Issue.\nUse the template feature_request.md.\nProvide a clear and descriptive title for the issue to identify the suggestion.\nProvide a step-by-step description of the suggested enhancement in as many details as possible.\nExplain why this enhancement would be useful to most tips-and-tricks users.\n\nWe will then take care of the issue as soon as possible."
  },
  {
    "objectID": "CONTRIBUTING.html#contributing-code",
    "href": "CONTRIBUTING.html#contributing-code",
    "title": "Contributing",
    "section": "",
    "text": "We use the GitHub flow to collaborate on this project:\n\nFork this repository using the GitHub interface.\nClone your fork using git clone fork-url (replace fork-url by the URL of your fork). Alternatively, open RStudio IDE and create a New Project from Version Control.\nCreate a new branch w/ git checkout -b branch-name (replace branch-name by the name of your new branch).\nMake your contribution (see below for examples).\nStage (git add) and commit (git commit) your changes as often as necessary\nPush your changes to GitHub w/ git push origin branch-name.\nSubmit a Pull Request on the original repo.\n\nWe will then review the PR as soon as possible.\n\n\n\n\n\nIf you want to contribute by improving the README, please edit the README.Rmd (not the README.md). Do not forget to update the README.md by running:\nrmarkdown::render(\"README.Rmd\")\n\n\n\nIf you want to contribute by improving the documentation of a function, open the corresponding file in the R/ folder and edit lines starting with #' (roxygen2 syntax).\nUpdate the documentation (Rd files in the man/ folder) by running:\ndevtools::document()\nIf you use a new external dependency in the example section, do not forget to add it in the DESCRIPTION file under the section Imports (only if this package is not already listed).\nCheck the integrity of the package with:\ndevtools::check()\n\n\n\n\nIf you want to contribute by improving the code of a function, open and edit the corresponding file in the R/ folder.\nCheck the integrity of the package with:\ndevtools::check()\nDo not forget to adapt the unit tests for the function by editing the corresponding file stored in the tests/testthat/ folder. We use the package testthat to implement unit tests.\nCheck your tests by running:\ndevtools::test()\n\n\n\nIf you want to contribute by writing a new post, please read this page.\nThanks for your contribution!"
  },
  {
    "objectID": "LICENSE.html",
    "href": "LICENSE.html",
    "title": "Attribution 4.0 International",
    "section": "",
    "text": "Attribution 4.0 International\nCreative Commons Corporation (“Creative Commons”) is not a law firm and does not provide legal services or legal advice. Distribution of Creative Commons public licenses does not create a lawyer-client or other relationship. Creative Commons makes its licenses and related information available on an “as-is” basis. Creative Commons gives no warranties regarding its licenses, any material licensed under their terms and conditions, or any related information. Creative Commons disclaims all liability for damages resulting from their use to the fullest extent possible.\nUsing Creative Commons Public Licenses\nCreative Commons public licenses provide a standard set of terms and conditions that creators and other rights holders may use to share original works of authorship and other material subject to copyright and certain other rights specified in the public license below. The following considerations are for informational purposes only, are not exhaustive, and do not form part of our licenses.\n Considerations for licensors: Our public licenses are\n intended for use by those authorized to give the public\n permission to use material in ways otherwise restricted by\n copyright and certain other rights. Our licenses are\n irrevocable. Licensors should read and understand the terms\n and conditions of the license they choose before applying it.\n Licensors should also secure all rights necessary before\n applying our licenses so that the public can reuse the\n material as expected. Licensors should clearly mark any\n material not subject to the license. This includes other CC-\n licensed material, or material used under an exception or\n limitation to copyright. More considerations for licensors:\nwiki.creativecommons.org/Considerations_for_licensors\n\n Considerations for the public: By using one of our public\n licenses, a licensor grants the public permission to use the\n licensed material under specified terms and conditions. If\n the licensor's permission is not necessary for any reason--for\n example, because of any applicable exception or limitation to\n copyright--then that use is not regulated by the license. Our\n licenses grant only permissions under copyright and certain\n other rights that a licensor has authority to grant. Use of\n the licensed material may still be restricted for other\n reasons, including because others have copyright or other\n rights in the material. A licensor may make special requests,\n such as asking that all changes be marked or described.\n Although not required by our licenses, you are encouraged to\n respect those requests where reasonable. More considerations\n for the public: \nwiki.creativecommons.org/Considerations_for_licensees\n=======================================================================\nCreative Commons Attribution 4.0 International Public License\nBy exercising the Licensed Rights (defined below), You accept and agree to be bound by the terms and conditions of this Creative Commons Attribution 4.0 International Public License (“Public License”). To the extent this Public License may be interpreted as a contract, You are granted the Licensed Rights in consideration of Your acceptance of these terms and conditions, and the Licensor grants You such rights in consideration of benefits the Licensor receives from making the Licensed Material available under these terms and conditions.\nSection 1 – Definitions.\n\nAdapted Material means material subject to Copyright and Similar Rights that is derived from or based upon the Licensed Material and in which the Licensed Material is translated, altered, arranged, transformed, or otherwise modified in a manner requiring permission under the Copyright and Similar Rights held by the Licensor. For purposes of this Public License, where the Licensed Material is a musical work, performance, or sound recording, Adapted Material is always produced where the Licensed Material is synched in timed relation with a moving image.\nAdapter’s License means the license You apply to Your Copyright and Similar Rights in Your contributions to Adapted Material in accordance with the terms and conditions of this Public License.\nCopyright and Similar Rights means copyright and/or similar rights closely related to copyright including, without limitation, performance, broadcast, sound recording, and Sui Generis Database Rights, without regard to how the rights are labeled or categorized. For purposes of this Public License, the rights specified in Section 2(b)(1)-(2) are not Copyright and Similar Rights.\nEffective Technological Measures means those measures that, in the absence of proper authority, may not be circumvented under laws fulfilling obligations under Article 11 of the WIPO Copyright Treaty adopted on December 20, 1996, and/or similar international agreements.\nExceptions and Limitations means fair use, fair dealing, and/or any other exception or limitation to Copyright and Similar Rights that applies to Your use of the Licensed Material.\nLicensed Material means the artistic or literary work, database, or other material to which the Licensor applied this Public License.\nLicensed Rights means the rights granted to You subject to the terms and conditions of this Public License, which are limited to all Copyright and Similar Rights that apply to Your use of the Licensed Material and that the Licensor has authority to license.\nLicensor means the individual(s) or entity(ies) granting rights under this Public License.\nShare means to provide material to the public by any means or process that requires permission under the Licensed Rights, such as reproduction, public display, public performance, distribution, dissemination, communication, or importation, and to make material available to the public including in ways that members of the public may access the material from a place and at a time individually chosen by them.\nSui Generis Database Rights means rights other than copyright resulting from Directive 96/9/EC of the European Parliament and of the Council of 11 March 1996 on the legal protection of databases, as amended and/or succeeded, as well as other essentially equivalent rights anywhere in the world.\nYou means the individual or entity exercising the Licensed Rights under this Public License. Your has a corresponding meaning.\n\nSection 2 – Scope.\n\nLicense grant.\n\nSubject to the terms and conditions of this Public License, the Licensor hereby grants You a worldwide, royalty-free, non-sublicensable, non-exclusive, irrevocable license to exercise the Licensed Rights in the Licensed Material to:\n\nreproduce and Share the Licensed Material, in whole or in part; and\nproduce, reproduce, and Share Adapted Material.\n\nExceptions and Limitations. For the avoidance of doubt, where Exceptions and Limitations apply to Your use, this Public License does not apply, and You do not need to comply with its terms and conditions.\nTerm. The term of this Public License is specified in Section 6(a).\nMedia and formats; technical modifications allowed. The Licensor authorizes You to exercise the Licensed Rights in all media and formats whether now known or hereafter created, and to make technical modifications necessary to do so. The Licensor waives and/or agrees not to assert any right or authority to forbid You from making technical modifications necessary to exercise the Licensed Rights, including technical modifications necessary to circumvent Effective Technological Measures. For purposes of this Public License, simply making modifications authorized by this Section 2(a)\n\nnever produces Adapted Material.\n\nDownstream recipients.\n\nOffer from the Licensor – Licensed Material. Every recipient of the Licensed Material automatically receives an offer from the Licensor to exercise the Licensed Rights under the terms and conditions of this Public License.\nNo downstream restrictions. You may not offer or impose any additional or different terms or conditions on, or apply any Effective Technological Measures to, the Licensed Material if doing so restricts exercise of the Licensed Rights by any recipient of the Licensed Material.\n\nNo endorsement. Nothing in this Public License constitutes or may be construed as permission to assert or imply that You are, or that Your use of the Licensed Material is, connected with, or sponsored, endorsed, or granted official status by, the Licensor or others designated to receive attribution as provided in Section 3(a)(1)(A)(i).\n\nOther rights.\n\nMoral rights, such as the right of integrity, are not licensed under this Public License, nor are publicity, privacy, and/or other similar personality rights; however, to the extent possible, the Licensor waives and/or agrees not to assert any such rights held by the Licensor to the limited extent necessary to allow You to exercise the Licensed Rights, but not otherwise.\nPatent and trademark rights are not licensed under this Public License.\nTo the extent possible, the Licensor waives any right to collect royalties from You for the exercise of the Licensed Rights, whether directly or through a collecting society under any voluntary or waivable statutory or compulsory licensing scheme. In all other cases the Licensor expressly reserves any right to collect such royalties.\n\n\nSection 3 – License Conditions.\nYour exercise of the Licensed Rights is expressly made subject to the following conditions.\n\nAttribution.\n\nIf You Share the Licensed Material (including in modified form), You must:\n\nretain the following if it is supplied by the Licensor with the Licensed Material:\n\nidentification of the creator(s) of the Licensed Material and any others designated to receive attribution, in any reasonable manner requested by the Licensor (including by pseudonym if designated);\na copyright notice;\na notice that refers to this Public License;\na notice that refers to the disclaimer of warranties;\na URI or hyperlink to the Licensed Material to the extent reasonably practicable;\n\nindicate if You modified the Licensed Material and retain an indication of any previous modifications; and\nindicate the Licensed Material is licensed under this Public License, and include the text of, or the URI or hyperlink to, this Public License.\n\nYou may satisfy the conditions in Section 3(a)(1) in any reasonable manner based on the medium, means, and context in which You Share the Licensed Material. For example, it may be reasonable to satisfy the conditions by providing a URI or hyperlink to a resource that includes the required information.\nIf requested by the Licensor, You must remove any of the information required by Section 3(a)(1)(A) to the extent reasonably practicable.\nIf You Share Adapted Material You produce, the Adapter’s License You apply must not prevent recipients of the Adapted Material from complying with this Public License.\n\n\nSection 4 – Sui Generis Database Rights.\nWhere the Licensed Rights include Sui Generis Database Rights that apply to Your use of the Licensed Material:\n\nfor the avoidance of doubt, Section 2(a)(1) grants You the right to extract, reuse, reproduce, and Share all or a substantial portion of the contents of the database;\nif You include all or a substantial portion of the database contents in a database in which You have Sui Generis Database Rights, then the database in which You have Sui Generis Database Rights (but not its individual contents) is Adapted Material; and\nYou must comply with the conditions in Section 3(a) if You Share all or a substantial portion of the contents of the database.\n\nFor the avoidance of doubt, this Section 4 supplements and does not replace Your obligations under this Public License where the Licensed Rights include other Copyright and Similar Rights.\nSection 5 – Disclaimer of Warranties and Limitation of Liability.\n\nUNLESS OTHERWISE SEPARATELY UNDERTAKEN BY THE LICENSOR, TO THE EXTENT POSSIBLE, THE LICENSOR OFFERS THE LICENSED MATERIAL AS-IS AND AS-AVAILABLE, AND MAKES NO REPRESENTATIONS OR WARRANTIES OF ANY KIND CONCERNING THE LICENSED MATERIAL, WHETHER EXPRESS, IMPLIED, STATUTORY, OR OTHER. THIS INCLUDES, WITHOUT LIMITATION, WARRANTIES OF TITLE, MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, NON-INFRINGEMENT, ABSENCE OF LATENT OR OTHER DEFECTS, ACCURACY, OR THE PRESENCE OR ABSENCE OF ERRORS, WHETHER OR NOT KNOWN OR DISCOVERABLE. WHERE DISCLAIMERS OF WARRANTIES ARE NOT ALLOWED IN FULL OR IN PART, THIS DISCLAIMER MAY NOT APPLY TO YOU.\nTO THE EXTENT POSSIBLE, IN NO EVENT WILL THE LICENSOR BE LIABLE TO YOU ON ANY LEGAL THEORY (INCLUDING, WITHOUT LIMITATION, NEGLIGENCE) OR OTHERWISE FOR ANY DIRECT, SPECIAL, INDIRECT, INCIDENTAL, CONSEQUENTIAL, PUNITIVE, EXEMPLARY, OR OTHER LOSSES, COSTS, EXPENSES, OR DAMAGES ARISING OUT OF THIS PUBLIC LICENSE OR USE OF THE LICENSED MATERIAL, EVEN IF THE LICENSOR HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH LOSSES, COSTS, EXPENSES, OR DAMAGES. WHERE A LIMITATION OF LIABILITY IS NOT ALLOWED IN FULL OR IN PART, THIS LIMITATION MAY NOT APPLY TO YOU.\nThe disclaimer of warranties and limitation of liability provided above shall be interpreted in a manner that, to the extent possible, most closely approximates an absolute disclaimer and waiver of all liability.\n\nSection 6 – Term and Termination.\n\nThis Public License applies for the term of the Copyright and Similar Rights licensed here. However, if You fail to comply with this Public License, then Your rights under this Public License terminate automatically.\nWhere Your right to use the Licensed Material has terminated under Section 6(a), it reinstates:\n\nautomatically as of the date the violation is cured, provided it is cured within 30 days of Your discovery of the violation; or\nupon express reinstatement by the Licensor.\n\nFor the avoidance of doubt, this Section 6(b) does not affect any right the Licensor may have to seek remedies for Your violations of this Public License.\nFor the avoidance of doubt, the Licensor may also offer the Licensed Material under separate terms or conditions or stop distributing the Licensed Material at any time; however, doing so will not terminate this Public License.\nSections 1, 5, 6, 7, and 8 survive termination of this Public License.\n\nSection 7 – Other Terms and Conditions.\n\nThe Licensor shall not be bound by any additional or different terms or conditions communicated by You unless expressly agreed.\nAny arrangements, understandings, or agreements regarding the Licensed Material not stated herein are separate from and independent of the terms and conditions of this Public License.\n\nSection 8 – Interpretation.\n\nFor the avoidance of doubt, this Public License does not, and shall not be interpreted to, reduce, limit, restrict, or impose conditions on any use of the Licensed Material that could lawfully be made without permission under this Public License.\nTo the extent possible, if any provision of this Public License is deemed unenforceable, it shall be automatically reformed to the minimum extent necessary to make it enforceable. If the provision cannot be reformed, it shall be severed from this Public License without affecting the enforceability of the remaining terms and conditions.\nNo term or condition of this Public License will be waived and no failure to comply consented to unless expressly agreed to by the Licensor.\nNothing in this Public License constitutes or may be interpreted as a limitation upon, or waiver of, any privileges and immunities that apply to the Licensor or You, including from the legal processes of any jurisdiction or authority.\n\n=======================================================================\nCreative Commons is not a party to its public licenses. Notwithstanding, Creative Commons may elect to apply one of its public licenses to material it publishes and in those instances will be considered the “Licensor.” The text of the Creative Commons public licenses is dedicated to the public domain under the CC0 Public Domain Dedication. Except for the limited purpose of indicating that material is shared under a Creative Commons public license or as otherwise permitted by the Creative Commons policies published at creativecommons.org/policies, Creative Commons does not authorize the use of the trademark “Creative Commons” or any other trademark or logo of Creative Commons without its prior written consent including, without limitation, in connection with any unauthorized modifications to any of its public licenses or any other arrangements, understandings, or agreements concerning use of licensed material. For the avoidance of doubt, this paragraph does not form part of the public licenses.\nCreative Commons may be contacted at creativecommons.org."
  },
  {
    "objectID": "posts/2024-03-26-zsh-an-alternative-to-bash/index.html",
    "href": "posts/2024-03-26-zsh-an-alternative-to-bash/index.html",
    "title": "ZSH: an alternative to Bash (Windows)",
    "section": "",
    "text": "ImportantWindows-only\n\n\n\nThis post only concerns Windows users."
  },
  {
    "objectID": "posts/2024-03-26-zsh-an-alternative-to-bash/index.html#introduction",
    "href": "posts/2024-03-26-zsh-an-alternative-to-bash/index.html#introduction",
    "title": "ZSH: an alternative to Bash (Windows)",
    "section": "Introduction",
    "text": "Introduction\nWhen you install git on Windows systems, you can also install the Git Bash software. It’s an interesting solution to run common Linux command lines (git, cd, rm, mkdir, nano, etc.) on Windows environments. Git Bash uses Bash as the default shell, but other shells, more powerful, are available.\nIn this post we will focus on the Z Shell (or zsh). It inherits Bash’s features and provides some improvements1:\n\ncommand suggestion\ncommand line completion\nfile globbing\nsyntax highlighting\nspelling correction\nthemeable prompts\nand many other plugins\n\nWe will also present Oh My Zsh!, an open source, community-driven framework for managing zsh plugins."
  },
  {
    "objectID": "posts/2024-03-26-zsh-an-alternative-to-bash/index.html#installation",
    "href": "posts/2024-03-26-zsh-an-alternative-to-bash/index.html#installation",
    "title": "ZSH: an alternative to Bash (Windows)",
    "section": "Installation",
    "text": "Installation\n\nZSH shell\nTo install zsh on Git Bash, proceed as follow:\n\nVisit this page and download the file zsh-X.X-X-x86_64.pkg.tar.zst.\nInstall the software PeaZip (required to extract the content of zst archives).\nExtract the content of zsh-X.X-X-x86_64.pkg.tar.zst with PeaZip.\nCopy all the files in the folder C:/Program Files/Git/.\nOpen Git Bash and run zsh (press q to skip the zsh configuration).\nDelete the zsh-X.X-X-x86_64.pkg.tar.zst file (and extracted files).\nUninstall PeaZip.\n\nNow we will set zsh as the default shell on Git Bash. To do so, we are going to edit/create the configuration file of the bash shell to tell Git Bash to launch zsh at startup.\nOpen Git Bash, run nano ~/.bashrc and add these lines:\nif [ -t 1 ]; then\n  exec zsh\nfi\nPress CTRL + X, Y, and Return to save and quit this file. Close and reopen Git Bash. zsh is now the default shell.\n\n\nOh My Zsh\nOh My Zsh! is a framework for making easier the configuration of zsh and managing zsh plugins. To install Oh My Zsh! run the following command on Git Bash:\nsh -c \"$(curl -fsSL https://raw.githubusercontent.com/ohmyzsh/ohmyzsh/master/tools/install.sh)\"\nThe prompt must have changed (-&gt; ~).\nN.B. You can ignore the error message (ERROR: this script is obsolete…)."
  },
  {
    "objectID": "posts/2024-03-26-zsh-an-alternative-to-bash/index.html#customization",
    "href": "posts/2024-03-26-zsh-an-alternative-to-bash/index.html#customization",
    "title": "ZSH: an alternative to Bash (Windows)",
    "section": "Customization",
    "text": "Customization\n\nColor theme\nYou can change the default color theme of Git Bash by right-clicking on the title bar and by selecting Options.... In the Look menu, you can select a predefined theme (e.g. Gruvbox).\n\n\nNerd Fonts\nBefore changing the zsh prompt, we need to install on Windows a Nerd Font. It’s recommended to use the MesloLG Nerd Font with the Powerlevel10k prompt (the prompt we will install later).\nProceed as follow:\n\nDownload the MesloLG Nerd Font pack at https://www.nerdfonts.com/font-downloads.\nExtract the content of the ZIP file.\nSelect all files (except the README.md and the LICENSE), right-click and select Install.\n\nThen, on Git Bash, right-click on the title bar and select Options.... In the Text menu, click on Select... and choose MesloLGS NF.\n\n\nPowerlevel10k prompt\nWe are going to install the Powerlevel10k prompt. To install this prompt with Oh My Zsh!, run this command:\ngit clone --depth=1 https://github.com/romkatv/powerlevel10k.git ${ZSH_CUSTOM:-$HOME/.oh-my-zsh/custom}/themes/powerlevel10k\nNow we need to activate this prompt in the configuration file of zsh (named ~/.zshrc). Let’s open this file with nano ~/.zshrc.\nReplace the line ZSH_THEME=\"robbyrussell\" by:\nZSH_THEME=\"powerlevel10k/powerlevel10k\"\nPress CTRL + X, Y, and Return to save and quit this file.\nFinally, run source ~/.zshrc to update the configuration of zsh. This will launch the Powerlevel10k configuration wizard. Answer the questions as you wish to customize the prompt.\nCongrats! Your prompt should look like this one:\n\n\n\n\n\n\n\n\n\n\n\nTipOh My Zsh! prompts\n\n\n\nYou can install a different prompt (theme) if you want. Oh My Zsh! provides a lot of different themes.\n\n\n\n\nZSH plugins\nOh My Zsh! comes bundled with many plugins that add new features to zsh shell. By default, the git plugin is enabled. It provides git aliases (shortcuts).\n\nz plugin\nLet’s add a new functionality by enabling the z plugin. This plugin defines the z command that tracks your most visited directories and allows you to access them with very few keystrokes.\nTo activate this plugin, we need to modify a line in the configuration file of zsh. Let’s open this file with nano ~/.zshrc.\nReplace the line plugins=(...) by:\nplugins=(... z)\nPress CTRL + X, Y, and Return to save and quit this file.\nFinally, run source ~/.zshrc to update the configuration of zsh.\n\n\n\n\n\n\nTipEnabling plugins\n\n\n\nYou can use the same method to activate other plugins bundled with Oh My Zsh!\n\n\nTo use the z command, you must first visit the directory with the Linux command cd. After a while, you will be able to access this directory by simply writing z followed by a pattern specific to this directory.\nFor example, if you visit the folder C:/Users/Username/Document/Projects a couple of times, you will able to write z proj. After pressing the TAB key, the command line will be replaced by: z C:/Users/Username/Document/Projects. Press Return and this will have the same effect as running cd C:/Users/Username/Document/Projects.\n\n\nAutosuggestions plugin\nThe zsh-autosuggestions plugin is a fast autosuggestions tool for zsh. It suggests commands as you type based on history and completions.\nThis plugin is not bundled with Oh My Zsh! but we can install it with git:\ngit clone https://github.com/zsh-users/zsh-autosuggestions.git ${ZSH_CUSTOM}/plugins/zsh-autosuggestions\nTo activate this plugin, we need to modify the configuration file of zsh. Let’s open this file with nano ~/.zshrc.\nReplace the line plugins=(...) by:\nplugins=(... zsh-autosuggestions)\nPress CTRL + X, Y, and Return to save and quit this file.\nFinally, run source ~/.zshrc to update the configuration of zsh.\nTo use this feature, start typing a command (for example cd) and you will see a completion offered after the cursor in a muted gray color. Press TAB to accept the suggestion or use the Up and Down arrow keys to navigate into the history associated with this command.\n\n\nSyntax highlighting plugin\nThe zsh-syntax-highlighting plugin is a tool that enables highlighting of commands as they are typed. Valid commands, errors, strings, and URL will be highlighted by a specific color.\nThis plugin is not bundled with Oh My Zsh! but we can install it with git:\ngit clone https://github.com/zsh-users/zsh-syntax-highlighting.git ${ZSH_CUSTOM}/plugins/zsh-syntax-highlighting\nTo activate this plugin, we need to modify the configuration file of zsh. Let’s open this file with nano ~/.zshrc.\nReplace the line plugins=(...) by:\nplugins=(... zsh-syntax-highlighting)\nPress CTRL + X, Y, and Return to save and quit this file.\nFinally, run source ~/.zshrc to update the configuration of zsh.\n\n\n\nAliases\nYou can improve your productivity by creating aliases (shortcuts). Note that this feature is available in many shells.\nFor example, we can create this alias:\nalias ..='cd ..'\nThen, instead of writing cd .., we can simply type .. to navigate to the parent directory.\nWe can store aliases in the configuration file of zsh. Let’s open this file with nano ~/.zshrc and add at the end of the file a subjective selection of aliases:\nalias ..='cd ..'\nalias gs='git status'\nalias ga='git add'\nalias gc='git commit -m'\nalias gp='git push'\nPress CTRL + X, Y, and Return to save and quit this file. Run source ~/.zshrc to update the configuration of zsh.\nOf course, you can add your own aliases. The hardest part is remembering!\n\n\n\n\n\n\nTipRStudio default terminal\n\n\n\nYou can benefit of all these features in RStudio IDE by changing the default terminal application. Go to Tools, Global Options and Terminal. Select Git Bash in New terminals open with."
  },
  {
    "objectID": "posts/2024-03-26-zsh-an-alternative-to-bash/index.html#footnotes",
    "href": "posts/2024-03-26-zsh-an-alternative-to-bash/index.html#footnotes",
    "title": "ZSH: an alternative to Bash (Windows)",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nhttps://en.wikipedia.org/wiki/Z_shell↩︎"
  },
  {
    "objectID": "posts/2024-03-12-storing-secrets-with-the-renviron-file/index.html",
    "href": "posts/2024-03-12-storing-secrets-with-the-renviron-file/index.html",
    "title": "Storing secrets with the .Renviron file",
    "section": "",
    "text": "What is a secret? It is usually a password, an username or an API token (key) required by services. For instance, some  packages require an authentication method:\n\nusethis requires a token to control the GitHub API\ntaxize requires tokens to query API provided by several taxonomic databases\nrwosstarter requires a token to access the Web of Science API\nrgbif requires an username and a password associated by a GBIF account\n\nIt’s tempting to record secrets as variables in R scripts. But keep in mind this: Never share your secrets with anyone (this also includes GitHub).\nAccording to the documentation of the httr package, there is three secure ways to store your secrets:\n\nAsk for the secret each time (no permanent storage).\nStore the secret as an environment variable.\nUse the keyring package and the operating system’s credential store.\n\nHere we will see how to easily store secret as an environment variable.\n\n\n\n\n\n\nTipEnvironment variable\n\n\n\nAccording to Wikipedia,\n\nAn environment variable is a user-definable value that can affect the way running processes will behave on a computer.\n\nIn short, it is a way to pass information to a program. Environment variable are used by many programs and operating systems and are made up of a name-value pair."
  },
  {
    "objectID": "posts/2024-03-12-storing-secrets-with-the-renviron-file/index.html#introduction",
    "href": "posts/2024-03-12-storing-secrets-with-the-renviron-file/index.html#introduction",
    "title": "Storing secrets with the .Renviron file",
    "section": "",
    "text": "What is a secret? It is usually a password, an username or an API token (key) required by services. For instance, some  packages require an authentication method:\n\nusethis requires a token to control the GitHub API\ntaxize requires tokens to query API provided by several taxonomic databases\nrwosstarter requires a token to access the Web of Science API\nrgbif requires an username and a password associated by a GBIF account\n\nIt’s tempting to record secrets as variables in R scripts. But keep in mind this: Never share your secrets with anyone (this also includes GitHub).\nAccording to the documentation of the httr package, there is three secure ways to store your secrets:\n\nAsk for the secret each time (no permanent storage).\nStore the secret as an environment variable.\nUse the keyring package and the operating system’s credential store.\n\nHere we will see how to easily store secret as an environment variable.\n\n\n\n\n\n\nTipEnvironment variable\n\n\n\nAccording to Wikipedia,\n\nAn environment variable is a user-definable value that can affect the way running processes will behave on a computer.\n\nIn short, it is a way to pass information to a program. Environment variable are used by many programs and operating systems and are made up of a name-value pair."
  },
  {
    "objectID": "posts/2024-03-12-storing-secrets-with-the-renviron-file/index.html#the-.renviron-file",
    "href": "posts/2024-03-12-storing-secrets-with-the-renviron-file/index.html#the-.renviron-file",
    "title": "Storing secrets with the .Renviron file",
    "section": "The .Renviron file",
    "text": "The .Renviron file\nThe  startup process is complex but one special  file is interesting for our purpose: the .Renviron file. This file does not contain  code: it only contains environment variables that will be pass to  at startup. It looks like:\nNAME1=value1\nNAME2=value2\nWe can use this file to store our secrets. To open (and create if necessary) the .Renviron file, run this command:\n\n## Create (if required) and open ~/.Renviron file ----\nutils::file.edit(\"~/.Renviron\")\n\n## Alternatively,\nusethis::edit_r_environ()\n\nThis command will create an .Renviron file at the user level (i.e. in his/her HOME directory) and all environment variables defined inside will be available for all  sessions.\nTo store a new secret, just add a new line. For instance:\nGITHUB_PAT='ghp_9999zzz9999zzz'\nN.B. add a new empty line at the end of this file otherwise  will ignore it."
  },
  {
    "objectID": "posts/2024-03-12-storing-secrets-with-the-renviron-file/index.html#accessing-secrets",
    "href": "posts/2024-03-12-storing-secrets-with-the-renviron-file/index.html#accessing-secrets",
    "title": "Storing secrets with the .Renviron file",
    "section": "Accessing secrets",
    "text": "Accessing secrets\nTo retrieve the value of a secret (and any environment variable), just use the function Sys.getenv().\n\n## Get secret value ----\nSys.getenv(\"GITHUB_PAT\")\n\n## Handle this secret securely ----\ngithub_pat &lt;- Sys.getenv(\"GITHUB_PAT\")\n\nN.B. by running Sys.getenv() without argument you will print all available environment variables."
  },
  {
    "objectID": "posts/2024-03-12-storing-secrets-with-the-renviron-file/index.html#to-go-further",
    "href": "posts/2024-03-12-storing-secrets-with-the-renviron-file/index.html#to-go-further",
    "title": "Storing secrets with the .Renviron file",
    "section": "To go further",
    "text": "To go further\nA .Renviron file can be created at three different levels:\n\nat the system level (named .Renviron.site)\nat the user level (~/.Renviron)\nat the project level (.Renviron)\n\nAt startup  will first read .Renviron.site, then ~/.Renviron and finally the .Renviron of the project. This means that if the same environment variable is defined in ~/.Renviron (user level) and in the .Renviron of the project, the value defined in the .Renviron of the project will overwrite the one defined at the user level.\n\nIf you want to store a secret that be shared among projects, store it in the ~/.Renviron (user level)\nIf you want to store a secret specific to a project, store it in the .Renviron (project level)\n\n\n\n\n\n\n\nImportantgit and .Renviron\n\n\n\nIf you create a .Renviron file at the project level, do not forget to add this file to the .gitignore. Otherwise your secret will be published on GitHub."
  },
  {
    "objectID": "posts/2024-03-12-storing-secrets-with-the-renviron-file/index.html#resources",
    "href": "posts/2024-03-12-storing-secrets-with-the-renviron-file/index.html#resources",
    "title": "Storing secrets with the .Renviron file",
    "section": "Resources",
    "text": "Resources\nhttps://rstats.wtf/r-startup.html https://resources.numbat.space/using-.rprofile-and-.html https://cran.r-project.org/web/packages/httr/vignettes/secrets.html https://www.r-bloggers.com/2024/02/key-advantages-of-using-the-keyring-package/"
  },
  {
    "objectID": "posts/2024-02-27-code-snippets-in-rstudio/index.html",
    "href": "posts/2024-02-27-code-snippets-in-rstudio/index.html",
    "title": "Code snippets in RStudio",
    "section": "",
    "text": "A code snippet is a text shortcut that is used to insert a predefined chunk of code. It’s like super-completion. The RStudio IDE contains a lot of predefined code snippets. For instance:\n\nsrc can be used to insert source(\"file.R\")\nlib can be used to insert library(package)\ntapply can be used to insert tapply(vector, index, function)\n\nTo use a snippet, enter the shortcut of the snippet (for example src) and press SHIFT + TAB to insert the corresponding chunk of code (for example source(\"file.R\")).\n\n\n\n\n\n\nNote\n\n\n\nCode snippets are language-specific meaning that they will work only with the appropriate language. For instance, in Markdown documents (.md, .Rmd and .qmd files), by using the snippet ![ (and SHIFT + TAB) you will write the markdown syntax to insert an image: ![label](location). This snippet won’t work inside .R files.\n\n\n\n\n\nFigure 1. How to use a snippet\n\n\nUse the key TAB to navigate inside the code snippet. In the snippet ![label](location), label and location are two variables. Once the snippet is created the cursor is in label. Use TAB to go to location.\nYou will find a list of predefined snippets in Tools &gt; Edit Code Snippets..."
  },
  {
    "objectID": "posts/2024-02-27-code-snippets-in-rstudio/index.html#using-snippets",
    "href": "posts/2024-02-27-code-snippets-in-rstudio/index.html#using-snippets",
    "title": "Code snippets in RStudio",
    "section": "",
    "text": "A code snippet is a text shortcut that is used to insert a predefined chunk of code. It’s like super-completion. The RStudio IDE contains a lot of predefined code snippets. For instance:\n\nsrc can be used to insert source(\"file.R\")\nlib can be used to insert library(package)\ntapply can be used to insert tapply(vector, index, function)\n\nTo use a snippet, enter the shortcut of the snippet (for example src) and press SHIFT + TAB to insert the corresponding chunk of code (for example source(\"file.R\")).\n\n\n\n\n\n\nNote\n\n\n\nCode snippets are language-specific meaning that they will work only with the appropriate language. For instance, in Markdown documents (.md, .Rmd and .qmd files), by using the snippet ![ (and SHIFT + TAB) you will write the markdown syntax to insert an image: ![label](location). This snippet won’t work inside .R files.\n\n\n\n\n\nFigure 1. How to use a snippet\n\n\nUse the key TAB to navigate inside the code snippet. In the snippet ![label](location), label and location are two variables. Once the snippet is created the cursor is in label. Use TAB to go to location.\nYou will find a list of predefined snippets in Tools &gt; Edit Code Snippets..."
  },
  {
    "objectID": "posts/2024-02-27-code-snippets-in-rstudio/index.html#customizing-snippets",
    "href": "posts/2024-02-27-code-snippets-in-rstudio/index.html#customizing-snippets",
    "title": "Code snippets in RStudio",
    "section": "Customizing snippets",
    "text": "Customizing snippets\nYou can easily add your own code snippets by editing the code snippets editor in Tools &gt; Edit Code Snippets...\n\n\n\nFigure 2. RStudio code snippets\n\n\nLet’s add a snippet that will create a new code section for us:\n\nsnippet sec\n    ## ${1:Title} ----\n    \n    ${0}\n\nThis snippet has two variables: ${1:Title} and ${0}.\nWhen we write sec and press SHIFT + TAB a new section will be created. The cursor will be on Title and after pressing TAB we go to the next line.\n\n\n\nFigure 3. Custom snippet in action\n\n\nWhen creating a new snippet, make sure to select the appropriate language (Figure 2).\n\n\n\n\n\n\nTip\n\n\n\n  For further information visit this article."
  },
  {
    "objectID": "posts/2026-02-19-dags/index.html",
    "href": "posts/2026-02-19-dags/index.html",
    "title": "DAGs",
    "section": "",
    "text": "Directd Acyclic Graphs (=DAG), also referred to as ‘causal models’, are graphical representations of the causal assumptions about a system. Nodes represent the variables and arrows represent assumed direct causal effects between variables. A DAG does not encode effect sizes or functional forms (e.g. linear vs. non-linear, quadratic..). It specifies only the assumed causal structure: which variables directly affect which others, and which do not. Importantly, the absence of an arrow is itself an assumption. Of course, additional information (e.g. timing or effect shape) can be added once the basic structure is defined.\nDAGs should include all causally relevant variables for the research question, including unmeasured ones. Depending on the purpose, DAGs can be quite simple as to highlight a specific relationship (Fig. 1a) or more comprehensive when reflecting a complex study system (Fig. 1b).\nImportantly, a DAG is not a statistical model (e.g. GLM, SEM, GAM) and does not estimate effects. It is a conceptual tool that helps clarifying one’s a priori assumptions about causal relationships.\n\n\n\n\n\n\nDisplay complex interactions A visual representation often makes it easier to understand a complex system. Identifying indirect pathways, missing or latent variables or potential lagged feedback structure will help to identify suitable analytical methods to correctly adressing the research question. In short: They help you make sense of your system\nClarify the role of each variable The visual representation helps identify whether variables act as confounder, collider or mediator. The relevance depends on the estimand (e.g. total vs. direct effect): falsely omitting confounders or including colliders as covariates in a model will lead to biased estimates if the focus is on estimating an exposure’s total effect. Applying covariate adjustment  helps identify the suitable minimal set of covariates, therefore can help to move from a ‘causal salad’ to thought-through effect estimation.\nInform downstream model choice and implementation  The causal structure clarified in a DAG can inform model specification. For example, by making unobserved confounders explicit it highlights which assumptions are strong and where effect estimation may fail. Covariate adjustment can help identify which variabes must be adjusted for. Importantly, the statistical model should reflect the causal model - not replace it.\n\n\n\n\n\nDisplay complex interactions See above. What helps you structuring your thoughts about a system will even more facilitate the understanding among colleagues who are less familiar with your study system!\nShare (model) assumptions A DAG makes the underlying assumptions about a study system explicit. Which variables were considered? Which were omitted - on purpose or accidentally? This prevents post-hoc tweaking by making assumptions explicit beforehand. DAGs thus improve rigour and transparency, and (may) open and facilitate the scientific debate."
  },
  {
    "objectID": "posts/2026-02-19-dags/index.html#directed-acyclic-graphs",
    "href": "posts/2026-02-19-dags/index.html#directed-acyclic-graphs",
    "title": "DAGs",
    "section": "",
    "text": "Directd Acyclic Graphs (=DAG), also referred to as ‘causal models’, are graphical representations of the causal assumptions about a system. Nodes represent the variables and arrows represent assumed direct causal effects between variables. A DAG does not encode effect sizes or functional forms (e.g. linear vs. non-linear, quadratic..). It specifies only the assumed causal structure: which variables directly affect which others, and which do not. Importantly, the absence of an arrow is itself an assumption. Of course, additional information (e.g. timing or effect shape) can be added once the basic structure is defined.\nDAGs should include all causally relevant variables for the research question, including unmeasured ones. Depending on the purpose, DAGs can be quite simple as to highlight a specific relationship (Fig. 1a) or more comprehensive when reflecting a complex study system (Fig. 1b).\nImportantly, a DAG is not a statistical model (e.g. GLM, SEM, GAM) and does not estimate effects. It is a conceptual tool that helps clarifying one’s a priori assumptions about causal relationships.\n\n\n\n\n\n\nDisplay complex interactions A visual representation often makes it easier to understand a complex system. Identifying indirect pathways, missing or latent variables or potential lagged feedback structure will help to identify suitable analytical methods to correctly adressing the research question. In short: They help you make sense of your system\nClarify the role of each variable The visual representation helps identify whether variables act as confounder, collider or mediator. The relevance depends on the estimand (e.g. total vs. direct effect): falsely omitting confounders or including colliders as covariates in a model will lead to biased estimates if the focus is on estimating an exposure’s total effect. Applying covariate adjustment  helps identify the suitable minimal set of covariates, therefore can help to move from a ‘causal salad’ to thought-through effect estimation.\nInform downstream model choice and implementation  The causal structure clarified in a DAG can inform model specification. For example, by making unobserved confounders explicit it highlights which assumptions are strong and where effect estimation may fail. Covariate adjustment can help identify which variabes must be adjusted for. Importantly, the statistical model should reflect the causal model - not replace it.\n\n\n\n\n\nDisplay complex interactions See above. What helps you structuring your thoughts about a system will even more facilitate the understanding among colleagues who are less familiar with your study system!\nShare (model) assumptions A DAG makes the underlying assumptions about a study system explicit. Which variables were considered? Which were omitted - on purpose or accidentally? This prevents post-hoc tweaking by making assumptions explicit beforehand. DAGs thus improve rigour and transparency, and (may) open and facilitate the scientific debate."
  },
  {
    "objectID": "posts/2026-02-19-dags/index.html#how-to-dag",
    "href": "posts/2026-02-19-dags/index.html#how-to-dag",
    "title": "DAGs",
    "section": "How to DAG:",
    "text": "How to DAG:\nIdeally, it is the first step of a project and proceeding any analysis. It can for example accompany the literature review in the beginning of the project, or the exchange with specialists. It further can (and should) be updated along when new findings appear. To get started, you don’t need any tools; a blank paper or screen is enough. However, special tools (e.g. dagitty, R/Phyton libraries) can be useful as the DAG gets complex and overflowing.\nWhile drawing, just keep in mind:\n\nInclude all variables (you can always create a second, reduced version\nAn arrow indicates a relationship between two variables, identifying exposure and outcome variable or bidirectionalilty but making no assumption about the form of that link (in next steps, you can of course add colors or labels to highlight shape of relationship or timelag).\nThe absence of a link is meaningful!\n\nIn ecology, our DAGs will most often be expert DAGs, based on expert knowledge. In case no previous knowledge is available, causal discovery algorithms are available to infer plausible ‘causal’ structure from (time series) data itself (e.g. PCMCI, CCM). However, they come with strong assumptions (e.g. no hidden confounding, stationarity) and do not replace domain knowledge. Generally, the causal assumptions encoded in a DAG cannot be tested using"
  },
  {
    "objectID": "posts/2026-02-19-dags/index.html#covariate-ajustment",
    "href": "posts/2026-02-19-dags/index.html#covariate-ajustment",
    "title": "DAGs",
    "section": "Covariate ajustment",
    "text": "Covariate ajustment\nCovariate adjustment, also back-door adjustment, is a basic, practical, method of causal inference aiming to remove confounding/biasing effects by identifying the appropriate set of variables to control for in a model (when estimating a ‘causal’ effect). The idea is to block non-causal pathways (i.e. backdor-pathways, those created by common causes/confounders) while keeping the causal pathway of interest open. In practice, this typically means adjusting for confounders (common causes of exposure and outcome). Mediators, which lie on the causal pathway, should not be adjusted for when estimating the total effect, as doing so blocks part of the effect of interest. Colliders, variables influenced by both exposure and outcome, should not be conditioned on, as this would open spurious associations and induce bias. The choice of covariates therefore is determined by the assumed causal structure and the specific estimand, not by statistical significance, automated model selection or simply availability of the data. It is possible, that for a given relationship of interest multiple (minimal) adjustment sets are possible. Tools such as DAGitty or R/Phyton libraries facilitate the identification of minimal adjustment set; Daggle provides some examples and exercises to get familiar with the backdoor-criteria.\n\n\n\nFig.2: a) Illustration of variable ‘M’ acting as mediator, collider or confounder in in the causal relationship between X and Y. b) identifies each type in a real world example. For example, when studying the total effect of ‘Protected Area’ on ‘Forest Species Abundance’, one would want to include ‘slope’ but omit ‘poaching’ and ‘carbon sequestration’ which might bias the estimation. Please note that these only are examples of confounder, mediator and collider in the displayed system (modified after Fig. 3 Arif & MacNeil (2023) )"
  },
  {
    "objectID": "posts/2026-02-19-dags/index.html#challenges",
    "href": "posts/2026-02-19-dags/index.html#challenges",
    "title": "DAGs",
    "section": "Challenges",
    "text": "Challenges\nBy definition, DAGs are ‘acyclic’. This can be challenging in ecology, where many systems show feedback loops. One way to overcome this is to use time-explicity DAGs which unfold the system over time (e.g. Xt -&gt; Yt+1 -&gt; Xt+2) which preserves acyclicity while representign dynamic feedback. Another option is to focus on one time point, which should be explicitly stated."
  },
  {
    "objectID": "posts/2026-02-19-dags/index.html#useful-ressources",
    "href": "posts/2026-02-19-dags/index.html#useful-ressources",
    "title": "DAGs",
    "section": "Useful ressources",
    "text": "Useful ressources\n\nLit\n\nArif & MacNeil (2023) Applying the structural causal model framework for observational causal inference in ecology. Ecol Monographs vol 23 10.1002/ecm.1554\nArif & MacNeil (2022) Predictive models aren’t for causal inference. Ecol. Letters vol 25 10.1111/ele.14033\nBorger & Ramesh (2023) Let’s DAG in: how directed acyclic graphs can help behavioural ecology be more transparent. Proc B vol 292 10.1098/rspb.2025.0963\n\n\n\nTools\n\nDAGitty.net (browser tool & R Package; draw DAG, determine role of covariates, identify (minimal) adjustment sets)\ndagR (R package to draw & identify (minimal) adjustment sets [less intuitive than DAGitty])\nDoWhy  (Phython library; draw DAG, determine role of covariates, identify (minimal) adjustment sets)\nDaggle (a shiny app to learn and train the identification of minimal adjustment sets; links towards further beginner & advanced ressources)"
  },
  {
    "objectID": "posts/2024-12-17-deploying-a-website-with-github-pages/index.html",
    "href": "posts/2024-12-17-deploying-a-website-with-github-pages/index.html",
    "title": "Deploying a website with GitHub Pages",
    "section": "",
    "text": "GitHub Pages is an option available at the repository level to host and publish HTML pages (single webpage, website, HTML presentation, blog, book, etc.) through GitHub.\n\n\n\n\n\n\nImportantImportant\n\n\n\nTo enable this option for a repository, you must be the owner of the repository (or admin if the repository is hosted on a GitHub organization) and the repository must be public (for GitHub free plan).\n\n\nOn the repository page, click on Settings (top navigation bar) and click on the section Pages (right side bar).\n\n\n\nSet up GitHub Pages\n\n\nUse the following settings:\n\nSelect Deploy from a branch in Source.\nSelect the main (or master) branch in Branch.\nSelect / (root) in Branch if the HTML file is at the root of the repository or /docs if the HTML file is inside a docs/ directory.\n\nAfter a few minutes, your HTML file will be available at:\nhttps://&lt;ghaccount&gt;.github.io/&lt;reponame&gt;/&lt;filename&gt;.html\nwhere:\n\n&lt;ghaccount&gt; is your GitHub account (or GitHub organization)\n&lt;reponame&gt; your repository name\n&lt;filename&gt; the name of the HTML file.\n\n\n\n\n\n\n\nTipShorten URL w/ index.html\n\n\n\nIf you have a single HTML file, you can save it as index.html. The URL will be shortened to:\nhttps://&lt;ghaccount&gt;.github.io/&lt;reponame&gt;/\nIf your website contains many HTML pages, the homepage should the index.html."
  },
  {
    "objectID": "posts/2024-12-17-deploying-a-website-with-github-pages/index.html#github-pages",
    "href": "posts/2024-12-17-deploying-a-website-with-github-pages/index.html#github-pages",
    "title": "Deploying a website with GitHub Pages",
    "section": "",
    "text": "GitHub Pages is an option available at the repository level to host and publish HTML pages (single webpage, website, HTML presentation, blog, book, etc.) through GitHub.\n\n\n\n\n\n\nImportantImportant\n\n\n\nTo enable this option for a repository, you must be the owner of the repository (or admin if the repository is hosted on a GitHub organization) and the repository must be public (for GitHub free plan).\n\n\nOn the repository page, click on Settings (top navigation bar) and click on the section Pages (right side bar).\n\n\n\nSet up GitHub Pages\n\n\nUse the following settings:\n\nSelect Deploy from a branch in Source.\nSelect the main (or master) branch in Branch.\nSelect / (root) in Branch if the HTML file is at the root of the repository or /docs if the HTML file is inside a docs/ directory.\n\nAfter a few minutes, your HTML file will be available at:\nhttps://&lt;ghaccount&gt;.github.io/&lt;reponame&gt;/&lt;filename&gt;.html\nwhere:\n\n&lt;ghaccount&gt; is your GitHub account (or GitHub organization)\n&lt;reponame&gt; your repository name\n&lt;filename&gt; the name of the HTML file.\n\n\n\n\n\n\n\nTipShorten URL w/ index.html\n\n\n\nIf you have a single HTML file, you can save it as index.html. The URL will be shortened to:\nhttps://&lt;ghaccount&gt;.github.io/&lt;reponame&gt;/\nIf your website contains many HTML pages, the homepage should the index.html."
  },
  {
    "objectID": "posts/2024-12-17-deploying-a-website-with-github-pages/index.html#personal-website",
    "href": "posts/2024-12-17-deploying-a-website-with-github-pages/index.html#personal-website",
    "title": "Deploying a website with GitHub Pages",
    "section": "Personal website",
    "text": "Personal website\nGitHub provides a special repository to host a user (or organization) website: &lt;ghaccount&gt;.github.io.\nBy creating the repository &lt;ghaccount&gt;.github.io, your personal website will be available at:\nhttps://&lt;ghaccount&gt;.github.io/"
  },
  {
    "objectID": "posts/2024-12-17-deploying-a-website-with-github-pages/index.html#resources",
    "href": "posts/2024-12-17-deploying-a-website-with-github-pages/index.html#resources",
    "title": "Deploying a website with GitHub Pages",
    "section": "Resources",
    "text": "Resources\nhttps://docs.github.com/en/pages/ https://docs.gitlab.com/ee/user/project/pages/"
  },
  {
    "objectID": "posts/2024-04-02-python-tutorial-part-1/index.html",
    "href": "posts/2024-04-02-python-tutorial-part-1/index.html",
    "title": "Python tutorial - Part 1",
    "section": "",
    "text": "This post has a dedicated presentation available here.\nThe source of the Quarto presentation is available on GitHub."
  },
  {
    "objectID": "posts/2024-09-24-research-compendium/index.html",
    "href": "posts/2024-09-24-research-compendium/index.html",
    "title": "Research compendium",
    "section": "",
    "text": "This post explains how to work with a research compendium. The goal of a research compendium is to provide a standard and easily recognizable way for organizing the digital materials of a project to enable others to inspect, reproduce, and extend the research (Marwick B et al. 2018). A research compendium follows three general principles:"
  },
  {
    "objectID": "posts/2024-09-24-research-compendium/index.html#foreward",
    "href": "posts/2024-09-24-research-compendium/index.html#foreward",
    "title": "Research compendium",
    "section": "Foreward",
    "text": "Foreward\nIn order to assist us in creating the structure of our working directory, we will use the  rcompendium package, developed by the author of this post. This package allows for the automation of creating files and directories specific to a research compendium (and a  package).\nThe package is released on the CRAN but we will install the development version from GitHub:\n\n## Install 'remotes' package ----\ninstall.packages(\"remotes\")\n\n## Install 'rcompendium' package from GitHub ----\nremotes::install_github(\"frbcesab/rcompendium\")\n\n## Attach 'rcompendium' package -----\nlibrary(\"rcompendium\")\n\n  If you encounter difficulties installing the package, please carefully read the Installation section of the README.\nOnce the package is installed, you need to run the set_credentials() function to store your personal information locally (first name, last name, email, ORCID, communication protocol with GitHub). This information will automatically populate certain files in the compendium. This function should only be used once.\n\n## Store personal information ----\nset_credentials(given    = \"Jane\",\n                family   = \"Doe\", \n                email    = \"jane.doe@mail.me\", \n                orcid    = \"0000-0000-0000-0000\", \n                protocol = \"ssh\")\n\nThis information has been copied to the clipboard. Paste its content into the file ~/.Rprofile (opened in RStudio using this function). This file is read every time  is opened, and its content will be accessible to the rcompendium package.\nRestart the  session (Session &gt; Restart R) and verify that your personal information is correctly accessible.\n\n## Retrieve email ----\ngetOption(\"email\")\n# [1] \"jane.doe@mail.me\"\n\n## Retrieve family name ----\ngetOption(\"family\")\n# [1] \"Doe\""
  },
  {
    "objectID": "posts/2024-09-24-research-compendium/index.html#rstudio-project",
    "href": "posts/2024-09-24-research-compendium/index.html#rstudio-project",
    "title": "Research compendium",
    "section": "RStudio project",
    "text": "RStudio project\nWhen you start a new project in  it is strongly recommended to use RStudio Projects.\n  Create a new RStudio Project: File &gt; New Project &gt; New Directory &gt; New Project and proceed as follow:\n\nchoose a name for your project (short and without whitespace)\nselect the location where the new project will be created\nuncheck all other boxes\nconfirm\n\n\n\n\n\n\n\n\n\n\n\n\nTipGood practice #1\n\n\n\nAlways work within an RStudio project. This has the advantage of simplifying file paths, especially with the here package and its here() function. The paths will always be constructed relative to the folder containing the .Rproj file (the project root). This is called a relative path.\n  Never use the setwd() function again.\n\n\n\n\n\n\n\n\nNoteTo go further\n\n\n\nIf you share your project on a cloud-based git repository (e.g. GitHub, GitLab, etc.) to collaborate, we recommend to add the .Rproj file to the .gitignore. The content of this .Rproj file can change between RStudio versions leading to unnecessary git conflicts. Listing the .Rproj file in the .gitignore will ensure that each user is working locally with its own version of this file.\n\n\n\n  Research compendium at this stage\npractice/                        # Root of the compendium\n|\n└─ practice.Rproj                # RStudio project file"
  },
  {
    "objectID": "posts/2024-09-24-research-compendium/index.html#readme",
    "href": "posts/2024-09-24-research-compendium/index.html#readme",
    "title": "Research compendium",
    "section": "README",
    "text": "README\nEvery project must contain a README file. It is the showcase of the project. The roles of a README are multiple:\n\nDescribe the project\nExplain its contents\nExplain how to install it\nExplain how to use it\n\nIt is a simple text file (plain text-based file) that can be written in plain text (README.txt), in simple Markdown (README.md), in R Markdown (README.Rmd), in Quarto (README.qmd), etc.\n  Here, you will create a README.md (simple Markdown) at the root of your project.\n  Use the utils::file.edit() function, which allows you to open a file in the RStudio editor. If the file doesn’t exist, it will also create it.\n\n## Add a README ----\nutils::file.edit(here::here(\"README.md\"))\n\n  Run this line of code in the console: here::here(\"README.md\") and try to understand what the here::here() function does.\n  Edit this README.md by adding the information that you find relevant.\nSuggestion  \n\n\n# Practice\n\nThis project contains files to create a simple **research compendium** as \npresented in the training course \n[Reproducible Research in Computational Ecology](https://rdatatoolbox.github.io).\n\n\n## Content\n\nThis project is structured as follow:\n\n- `README.md`: presentation of the project\n- `practice.Rproj`: RStudio project file\n\n\n## Installation\n\nComing soon...\n\n\n## Usage\n\nComing soon...\n\n\n## Citation\n\n&gt; Doe J (2024) Minimal structure of a research compendium.\n\n\n\n\n\n\n\n\nTipGood practice #2\n\n\n\nAlways add a README to help the user understand your project. If you want to execute  code inside, write it in R Markdown (README.Rmd) or Quarto (README.qmd), otherwise, simply use basic Markdown (README.md).\nNB. If you write a .Rmd or .qmd, don’t forget to convert it into a .md file. GitHub can only interpret basic Markdown.\n\n## Convert .Rmd in .md ----\nrmarkdown::render(\"README.Rmd\")\n\n## Convert .qmd in .md ----\nquarto::quarto_render(\"README.qmd\")\n\n  You can also click on the Render button of RStudio.\n\n\n\n\n\n\n\n\nNoteThe rcompendium::add_readme_rmd() function\n\n\n\nYou can use the add_readme_rmd() function of rcompendium package that populates a README template for  projects.\n\n\n\n  Research compendium at this stage\npractice/                        # Root of the compendium\n|\n├─ practice.Rproj                # RStudio project file\n|\n└─ README.md                     # Presentation of the project"
  },
  {
    "objectID": "posts/2024-09-24-research-compendium/index.html#description",
    "href": "posts/2024-09-24-research-compendium/index.html#description",
    "title": "Research compendium",
    "section": "DESCRIPTION",
    "text": "DESCRIPTION\nThe DESCRIPTION file describes the metadata of the project (title, author, description, dependencies, etc.). It is one of the essential elements of a  package. Here, we will repurpose it for use in a research compendium in order to take advantage of package development tools (see below).\n  Add a DESCRIPTION file using the add_description() function of the rcompendium package.\n\n## Add a DESCRIPTION file ----\nrcompendium::add_description()\n\n\nPackage: practice\nType: Package\nTitle: The Title of the Project\nVersion: 0.0.0.9000\nAuthors@R: c(\n    person(given   = \"Jane\",\n           family  = \"Doe\",\n           role    = c(\"aut\", \"cre\", \"cph\"),\n           email   = \"jane.doe@mail.me\",\n           comment = c(ORCID = \"0000-0000-0000-0000\")))\nDescription: A paragraph providing a full description of the project (on \n    several lines...)\nLicense: {{license}}\nEncoding: UTF-8\n\nAs you can see, the DESCRIPTION file has been pre-filled with your personal information. You will edit the Title and Description fields later.\n\n\n\n\n\n\nTipGood practice #3\n\n\n\nAlways add a DESCRIPTION file at the root of the project. It is used to describe the project’s metadata: title, author(s), description, license, etc. We will discuss this later, but it is also the ideal place to list required external packages.\n\n\n\n  Research compendium at this stage\npractice/                        # Root of the compendium\n|\n├─ practice.Rproj                # RStudio project file\n|\n├─ README.md                     # Presentation of the project\n└─ DESCRIPTION                   # Project metadata"
  },
  {
    "objectID": "posts/2024-09-24-research-compendium/index.html#license",
    "href": "posts/2024-09-24-research-compendium/index.html#license",
    "title": "Research compendium",
    "section": "LICENSE",
    "text": "LICENSE\nAny material shared online must have a LICENSE that describes what can be done with it. Therefore, we recommend adding a license to your project from the start. To determine which license is most appropriate for your project, you can visit this website: https://choosealicense.com.\n  Add the GPL-3 license to your project using the add_license() function of the rcompendium package.\n\n## Add a license ----\nadd_license(license = \"GPL-3\")\n\nNote that a new file has been created: LICENSE.md. This file details the contents of the license and will be read by GitHub. Also, check the content of the DESCRIPTION file: the License section has been updated thanks to rcompendium.\n  Add a section in the README.md mentioning the license.\nSuggestion  \n\n\n# Practice\n\nThis project contains files to create a simple **research compendium** as \npresented in the training course \n[Reproducible Research in Computational Ecology](https://rdatatoolbox.github.io).\n\n\n## Content\n\nThis project is structured as follow:\n\n- `README.md`: presentation of the project\n- `DESCRIPTION`: project metadata\n- `LICENSE.md`: license of the project\n- `practice.Rproj`: RStudio project file\n\n\n## Installation\n\nComing soon...\n\n\n## Usage\n\nComing soon...\n\n\n## License\n\nThis project is released under the \n[GPL-3](https://choosealicense.com/licenses/gpl-3.0/) license.\n\n\n## Citation\n\n&gt; Doe J (2024) Minimal structure of a research compendium.\n\n\n\n\n\n\n\n\nTipGood practice #4\n\n\n\nAlways add a LICENSE to a project that will be made public. Visit the Choose a License website to select the most appropriate one for your project.\nNote: If no license is provided, your project will be subject to the No License rules: no permissions are granted. In other words, no one can do anything with your project (no reuse, no modification, no sharing, etc.).\n\n\n\n  Research compendium at this stage\npractice/                        # Root of the compendium\n|\n├─ practice.Rproj                # RStudio project file\n|\n├─ README.md                     # Presentation of the project\n├─ DESCRIPTION                   # Project metadata\n└─ LICENSE.md                    # License of the project"
  },
  {
    "objectID": "posts/2024-09-24-research-compendium/index.html#subdirectories",
    "href": "posts/2024-09-24-research-compendium/index.html#subdirectories",
    "title": "Research compendium",
    "section": "Subdirectories",
    "text": "Subdirectories\nThe next step involves creating subdirectories, each with a specific role. The idea here is to separate the data, results, and code.\n  To do this, use the add_compendium() function from rcompendium.\n\n## Create subdirectories ----\nrcompendium::add_compendium()\n\n\n\n\n\n\n\nTipGood practice #5\n\n\n\nA good Research compendium will consist of different subdirectories, each intended to hold a specific type of file. By default, the add_compendium() function will create this organization:\n\nThe data/ folder will contain all the raw data necessary for the project.\nThe outputs/ folder will contain all the generated results (excluding figures).\nThe figures/ folder will contain all the figures produced by the analyses.\nThe R/ folder will only contain  functions (and their documentation). See below for more details.\nThe analyses/ folder will contain  scripts (or .Rmd and/or .qmd files) that will call the functions.\n\nNote: This structure can of course be adapted based on needs, personal practices, and the complexity of the project. With the exception of the R/ folder, all other directories can be named differently.\n\n\n\n  Research compendium at this stage\npractice/                        # Root of the compendium\n|\n├─ practice.Rproj                # RStudio project file\n|\n├─ README.md                     # Presentation of the project\n├─ DESCRIPTION                   # Project metadata\n├─ LICENSE.md                    # License of the project\n|\n├─ data/                         # Contains raw data\n├─ outputs/                      # Contains results\n├─ figures/                      # Contains figures\n├─ R/                            # Contains R functions (only)\n└─ analyses/                     # Contains R scripts"
  },
  {
    "objectID": "posts/2024-09-24-research-compendium/index.html#writing-code",
    "href": "posts/2024-09-24-research-compendium/index.html#writing-code",
    "title": "Research compendium",
    "section": "Writing code",
    "text": "Writing code\nWe’re ready to code!\n\nHere we will write a code that will download the PanTHERIA dataset (Jones et al. 2009) and save it locally in our compendium.\n\nPanTHERIA is a species-level database of life history, ecology, and geography of extant and recently extinct mammals. Metadata can be found here. Note that missing values are coded -999.\nWe’ll start by writing our code in a  script. The PanTHERIA data file, available here, will be saved in the data/pantheria/ subdirectory.\n  Create the download-data.R script in the analyses/ directory using the utils::file.edit() function.\n\n## Create a R script in the directory analyses/ ----\nutils::file.edit(here::here(\"analyses\", \"download-data.R\"))\n\n  Now write the code to download the data file.\n  Use dir.create() to create the subdirectory data/pantheria/, here::here() to build robust paths and utils::download.file() to download the file from the URL.\nSuggestion  \n\n\n# Download PanTHERIA dataset\n#\n# Author: Jane Doe\n# Date: 2024/09/24\n\n## Destination path ---- \npath &lt;- here::here(\"data\", \"pantheria\")\n\n## Create destination directory ----\ndir.create(path, showWarnings = FALSE, recursive = TRUE)\n\n## File name ----\nfilename &lt;- \"PanTHERIA_1-0_WR05_Aug2008.txt\"\n  \n## Repo base URL ----\nbase_url &lt;- \"https://esapubs.org/archive/ecol/E090/184/\"\n\n## Build full URL ----\nfull_url &lt;- paste0(base_url, filename)\n\n## Build full path ----\ndest_file &lt;- file.path(path, filename)\n\n## Download file ----\nutils::download.file(url      = full_url,\n                     destfile = dest_file,\n                     mode     = \"wb\")\n\n\n\n\n\n\n\n\nTipGood practice #6\n\n\n\nTry scripting the whole project (including data acquisition). Here, we’ve seen how to create files (utils::file.edit()) and directories (dir.create()), build robust relative paths (here::here()) and download files (utils::download.file()) directly from .\n\n\n\n\n\n\n\n\nNoteExternal packages\n\n\n\nTo use a function from an external package, you’ve learned to use library(pkg). In , there’s another syntax for calling a function from an external package: pkg::fun(). Whereas library() loads and attaches a package (making its functions directly accessible with fun()), the syntax pkg::fun() only loads a package in the session, but does not attach its contents. This means you have to specify the package name when calling the function.\nWe recommend using the pkg::fun() syntax. There are two reasons for this:\n\nA better code readability: at a glance, you’ll know which package the function is in.\nLimits conflicts between packages: two functions can have the same name in two different packages. For example, the dplyr package offers a filter() function which is also found in the stats package (attached to the opening of ). However, the filter() functions in these two packages do not do the same thing.\n\n\nlibrary(\"dplyr\")\n\n## Attaching package: ‘dplyr’\n## \n## The following objects are masked from ‘package:stats’:\n## \n##     filter, lag\n## \n## The following objects are masked from ‘package:base’:\n## \n##     intersect, setdiff, setequal, union\n\nIf you use library(dplyr), you’ll never be 100% sure whether you’re using the filter() function of the dplyr package or that of the stats package.\nHowever, for very verbose packages (such as ggplot2), you can use the library() function, otherwise your code will quickly become tedious to write.\n  If you wish to use the %&gt;% pipe, attach the magrittr package with library(magrittr).\n\n\n\n  Research compendium at this stage\npractice/                                   # Root of the compendium\n|\n├─ practice.Rproj                           # RStudio project file\n|\n├─ README.md                                # Presentation of the project\n├─ DESCRIPTION                              # Project metadata\n├─ LICENSE.md                               # License of the project\n|\n├─ data/                                    # Contains raw data\n|  └─ pantheria/                            # PanTHERIA database\n|     └─ PanTHERIA_1-0_WR05_Aug2008.txt\n|\n├─ outputs/                                 # Contains results\n├─ figures/                                 # Contains figures\n├─ R/                                       # Contains R functions (only)\n|\n└─ analyses/                                # Contains R scripts\n   └─ download-data.R                       # Script to download raw data"
  },
  {
    "objectID": "posts/2024-09-24-research-compendium/index.html#code-refactoring",
    "href": "posts/2024-09-24-research-compendium/index.html#code-refactoring",
    "title": "Research compendium",
    "section": "Code refactoring",
    "text": "Code refactoring\nWe can take this a step further by converting the script into function: this is known as code refactoring. A function is a set of lines of code grouped together in a single block to perform a specific task. Writing functions will make your code clearer and more easily reusable between projects.\n\n\n\n\n\n\nImportantConvention\n\n\n\nAlways store your  functions (and only functions) in a directory named R/ located at the root of the project.\n\n\n  Convert the previous  code into a function named dl_pantheria_data().\n  Use the usethis::use_r() function to create the function file inside the R/ directory.\n\n## Create the function file in R/ ----\nusethis::use_r(\"dl_pantheria_data\")\n\nSuggestion  \n\n\ndl_pantheria_data &lt;- function() {\n  \n  ## Destination path ---- \n  path &lt;- here::here(\"data\", \"pantheria\")\n  \n  ## Create destination directory ----\n  dir.create(path, showWarnings = FALSE, recursive = TRUE)\n  \n  ## File name ----\n  filename &lt;- \"PanTHERIA_1-0_WR05_Aug2008.txt\"\n    \n  ## Repo base URL ----\n  url &lt;- \"https://esapubs.org/archive/ecol/E090/184/\"\n  \n  ## Build full URL ----\n  full_url &lt;- paste0(base_url, filename)\n  \n  ## Build full path ----\n  dest_file &lt;- file.path(path, filename)\n\n  ## Download file ----\n  utils::download.file(url      = full_url,\n                       destfile = dest_file,\n                       mode     = \"wb\")\n  \n  return(dest_file)\n}\n\n\n\n\n\n\n\n\nTipGood practice #7\n\n\n\nWrite functions: this is called code refactoring. This will make your code clearer and easier to reuse. Always store your  functions in the R/ folder. If you’re using functions from external packages, write them as follows: pkg::fun().\n\n\n  Finally adapt the content of the analyses/download-data.R  script created earlier so that it calls the dl_pantheria_data() function.\nSuggestion  \n\n\n# Download project raw data\n#\n# This script will download the PanTHERIA dataset and will store it in `data/`\n# by calling the dl_pantheria_data() function available in the `R/` directory.\n#\n# Author: Jane Doe\n# Date: 2024/09/24\n\n## Download PanTHERIA dataset ----\npantheria_path  &lt;- dl_pantheria_data()\n\n\n\n\n\n\n\n\nTipGood practice #8\n\n\n\nThe analyses/ directory contains  scripts that call  functions stored in the R/ folder. In the case of complex analyses, don’t hesitate to multiply the scripts (rather than having a single large script).\n\n\n\n\n\n%%{init:{'theme':'neutral','flowchart':{'htmlLabels':false}}}%%\nflowchart LR\n  B(\"analyses/download-data.R\")\n  B --&gt; C(\"dl_pantheria_data()\")\n\n\n\n\n\n\n\n\n\n  Research compendium at this stage\npractice/                                   # Root of the compendium\n|\n├─ practice.Rproj                           # RStudio project file\n|\n├─ README.md                                # Presentation of the project\n├─ DESCRIPTION                              # Project metadata\n├─ LICENSE.md                               # License of the project\n|\n├─ data/                                    # Contains raw data\n|  └─ pantheria/                            # PanTHERIA database\n|     └─ PanTHERIA_1-0_WR05_Aug2008.txt\n|\n├─ outputs/                                 # Contains results\n├─ figures/                                 # Contains figures\n|\n├─ R/                                       # Contains R functions (only)\n|  └─ dl_pantheria_data.R                   # Function to download PanTHERIA data\n|\n└─ analyses/                                # Contains R scripts\n   └─ download-data.R                       # Script to download raw data"
  },
  {
    "objectID": "posts/2024-09-24-research-compendium/index.html#documentation",
    "href": "posts/2024-09-24-research-compendium/index.html#documentation",
    "title": "Research compendium",
    "section": "Documentation",
    "text": "Documentation\nIt’s time to document your function. It’s essential! To do this, we’re going to use the roxygen2 syntax. This makes it easy to document functions by placing a special header before the function. This header must contain (as a minimum) a title, a description of each argument and the function’s return.\n  Add a roxygen2 header to your function to document it.\nSuggestion  \n\n\n#' Download PanTHERIA dataset\n#'\n#' @description \n#' This function downloads the PanTHERIA dataset (text file) available at\n#' &lt;https://esapubs.org/archive/ecol/E090/184/PanTHERIA_1-0_WR05_Aug2008.txt&gt;.\n#' \n#' The file `PanTHERIA_1-0_WR05_Aug2008.txt` will be stored in \n#' `data/pantheria/`. Note that this folder will be created if required.\n#'\n#' @return This function returns the path (`character`) to the downloaded file\n#' (e.g. `data/pantheria/PanTHERIA_1-0_WR05_Aug2008.txt`).\n\ndl_pantheria_data &lt;- function() { ... }\n\n\n  Our function does not contain any parameter. But if this were the case, we would have had to describe the parameters with the roxygen2 tag #' @param.\n\n\n\n\n\n\nTipGood practice #9\n\n\n\nThink of others (and of your future self)! Always document your code. Code without documentation is useless. Use roxygen2 headers to document your  functions, simple comments to document code and README for everything else.\n\n\n\n\n\n\n\n\nNoteTo go further\n\n\n\nYou can convert your roxygen2 headers into .Rd files, the only files accepted by  for documenting functions. These .Rd files will be stored in the man/ folder. This is not mandatory when working with a research compendium but this is required if you develop a  package.\n\n## Generate function documentation (.Rd files) ----\ndevtools::document()\n\nHelp for your function will be available via ?fun_name.\n\n\n\n  Research compendium at this stage (same as before)\npractice/                                   # Root of the compendium\n|\n├─ practice.Rproj                           # RStudio project file\n|\n├─ README.md                                # Presentation of the project\n├─ DESCRIPTION                              # Project metadata\n├─ LICENSE.md                               # License of the project\n|\n├─ data/                                    # Contains raw data\n|  └─ pantheria/                            # PanTHERIA database\n|     └─ PanTHERIA_1-0_WR05_Aug2008.txt\n|\n├─ outputs/                                 # Contains results\n├─ figures/                                 # Contains figures\n|\n├─ R/                                       # Contains R functions (only)\n|  └─ dl_pantheria_data.R                   # Function to download PanTHERIA data\n|\n└─ analyses/                                # Contains R scripts\n   └─ download-data.R                       # Script to download raw data"
  },
  {
    "objectID": "posts/2024-09-24-research-compendium/index.html#dependencies",
    "href": "posts/2024-09-24-research-compendium/index.html#dependencies",
    "title": "Research compendium",
    "section": "Dependencies",
    "text": "Dependencies\nOur project depends on two external packages: utils and here. As mentioned previously, the DESCRIPTION file is the ideal place to centralize the list of required packages.\n  Add these two dependencies to the DESCRIPTION file with the usethis::use_package() function.\n\n## Add dependencies in DESCRIPTION ----\nusethis::use_package(package = \"here\")\nusethis::use_package(package = \"utils\")\n\nLook at the contents of the DESCRIPTION file: the two required packages are listed in the Imports section.\n\nPackage: practice\nType: Package\nTitle: The Title of the Project\nVersion: 0.0.0.9000\nAuthors@R: c(\n    person(given   = \"Jane\",\n           family  = \"Doe\",\n           role    = c(\"aut\", \"cre\", \"cph\"),\n           email   = \"jane.doe@mail.me\",\n           comment = c(ORCID = \"0000-0000-0000-0000\")))\nDescription: A paragraph providing a full description of the project (on \n    several lines...)\nLicense: GPL-3\nEncoding: UTF-8\nImports:\n    here,\n    utils\n\n\n\n\n\n\n\nTipGood practice #10\n\n\n\nAlways list the required packages in the DESCRIPTION file. In this way, you will centralize the list of required packages in one place and use the devtools::install_deps() and devtools::load_all() functions (see section Loading the project).\n\n\n\n\n\n\n\n\nNoteTo go further\n\n\n\nIf in your  code you want to attach your packages with library(), use the usethis::use_package() function as follows:\n\n## Create a strong dependency ----\nusethis::use_package(package = \"ggplot2\", type = \"Depends\")\n\nThe package will be added to the Depends section of the DESCRIPTION file."
  },
  {
    "objectID": "posts/2024-09-24-research-compendium/index.html#loading-the-project",
    "href": "posts/2024-09-24-research-compendium/index.html#loading-the-project",
    "title": "Research compendium",
    "section": "Loading the project",
    "text": "Loading the project\nNow that our compendium contains a DESCRIPTION file with a list of packages, we can use the package development tools  available in the package devtools to:\n1) Install packages with the devtools::install_deps() function\nThis function reads the DESCRIPTION file to retrieve packages listed in the Depends and Imports sections and install them (only if they are not already installed). This function therefore replaces the install.packages() function.\n  By default, this function will also ask you to update packages (if a new version is available). If you wish to disable this feature, add the argument upgrade = \"never\".\n2) Load packages with the devtools::load_all() function\nThis function will read the DESCRIPTION file to retrieve packages listed in the Depends and Imports sections. It will load the packages listed in the Imports section and load and attach the packages listed in the Depends section. This function therefore replaces the library() function.\n\n\n\n\n\n\nImportantImportant\n\n\n\nUpdate your DESCRIPTION file regularly by:\n\nadding any new packages you use\nremoving packages you no longer use\n\n\n\n3) Load functions   with the devtools::load_all() function\nThe devtools::load_all() function has a second advantage: it will load  functions stored in the R/ folder and make them accessible in the session. It therefore replaces the source() function.\n  After each modification to a  function, don’t forget to execute the devtools::load_all() function. You can use the keyboard shortcut Ctrl + Shift + L in RStudio.\n  Try these two functions.\n\n## Install required packages ----\ndevtools::install_deps(upgrade = \"never\")\n\n## Load packages and functions ----\ndevtools::load_all()\n\n\n\n\n\n\n\nTipGood practice #11\n\n\n\nWith a DESCRIPTION file (listing the required packages) and a R/ folder, you can use:\n\ndevtools::install_deps() to install (and update) packages: don’t use install.packages() anymore.\ndevtools::load_all() to 1) load (and attach) packages and 2) load your  functions: no longer use library() or source() (to load your functions)."
  },
  {
    "objectID": "posts/2024-09-24-research-compendium/index.html#main-script",
    "href": "posts/2024-09-24-research-compendium/index.html#main-script",
    "title": "Research compendium",
    "section": "Main script",
    "text": "Main script\nTo automate our project, we’ll create a main  script at the root of the project. By convention, we’ll call it make.R. It will have two objectives:\n\nset up the project by installing and loading packages and functions\nrun the project by sourcing scripts  sequentially.\n\nThe idea is that, once the project is finished, the user only executes this script: it’s the conductor of the project.\n  Use the utils::file.edit() function to create a  script at the root of the project.\n\n## Create a main script ----\nutils::file.edit(here::here(\"make.R\"))\n\n  Add the two previous functions:\n\n\n# Setup project ----\n\n## Install packages ----\ndevtools::install_deps(upgrade = \"never\")\n\n## Load packages & functions ----\ndevtools::load_all()\n\n\n  Finally, add a line to the make.R file that will execute the analyses/download-data.R script.\n  Use the source() and here::here() functions to do this.\nSuggestion  \n\n\n# Project title\n#\n# Project description\n# ...\n#\n# Author: Jane Doe\n# Date: 2024/12/02\n\n\n# Setup project ----\n\n## Install packages ----\ndevtools::install_deps(upgrade = \"never\")\n\n## Load packages & functions ----\ndevtools::load_all()\n\n\n# Run project ----\n\n## Download raw data ----\nsource(here::here(\"analyses\", \"download-data.R\"))\n\n\n\n\n\n\n\n\nTipGood practice #12\n\n\n\nA make.R file placed at the root of the project makes it easy to set up the project (install and load the required packages and  functions) and run the various analyses sequentially (by sourcing  scripts which themselves call  functions). This is the conductor of the project.\nNote: Given the simplicity of this project, we could easily have placed the contents of the  script (analyses/download-data.R) in this make.R. The structure of a compendium is not fixed, but we recommend that you use at least  functions and a make.R.\n\n\n\n\n\n%%{init:{'theme':'neutral','flowchart':{'htmlLabels':false}}}%%\nflowchart LR\n  A(\"make.R\") --&gt; B(\"analyses/download-data.R\")\n  B --&gt; C(\"dl_pantheria_data()\")\n\n\n\n\n\n\n\n\n\n  Research compendium at the end\npractice/                                   # Root of the compendium\n|\n├─ practice.Rproj                           # RStudio project file\n|\n├─ README.md                                # Presentation of the project\n├─ DESCRIPTION                              # Project metadata\n├─ LICENSE.md                               # License of the project\n|\n├─ data/                                    # Contains raw data\n|  └─ pantheria/                            # PanTHERIA database\n|     └─ PanTHERIA_1-0_WR05_Aug2008.txt\n|\n├─ outputs/                                 # Contains results\n├─ figures/                                 # Contains figures\n|\n├─ R/                                       # Contains R functions (only)\n|  └─ dl_pantheria_data.R                   # Function to download PanTHERIA data\n|\n├─ analyses/                                # Contains R scripts\n|  └─ download-data.R                       # Script to download raw data\n|\n└─ make.R                                   # Script to setup & run the project"
  },
  {
    "objectID": "posts/2024-09-24-research-compendium/index.html#documentation-again",
    "href": "posts/2024-09-24-research-compendium/index.html#documentation-again",
    "title": "Research compendium",
    "section": "Documentation (again)",
    "text": "Documentation (again)\nDon’t forget to finalize your project documentation.\n  Edit the Title and Description sections of the DESCRIPTION file.\n\nPackage: practice\nType: Package\nTitle: Download PanTHERIA database\nVersion: 0.0.0.9000\nAuthors@R: c(\n    person(given   = \"Jane\",\n           family  = \"Doe\",\n           role    = c(\"aut\", \"cre\", \"cph\"),\n           email   = \"jane.doe@mail.me\",\n           comment = c(ORCID = \"0000-0000-0000-0000\")))\nDescription: This project aims to download the PanTHERIA databases. It is \n    structured as a research compendium to be reproducible.\n    This is the result of the Practice 1 of the training course Reproducible \n    Research in Computational Ecology available at:\n    &lt;https://rdatatoolbox.github.io/chapters/ex-compendium.html&gt;.\nLicense: GPL-3\nEncoding: UTF-8\nImports:\n    here,\n    utils\n\n  Finally edit the README:\n\n# Practice\n\nThis project aims to download the [PanTHERIA](https://doi.org/10.1890/08-1494.1) \ndatabase (Jones _et al._, 2009). It is structured as a research compendium \nto be reproducible.\n\n**NB.** This is the result of the Practice 1 of the training course\n[Reproducible Research in Computational Ecology](https://rdatatoolbox.github.io).\n\n\n## Content\n\nThis project is structured as follow:\n\n.\n|\n├─ practice.Rproj                           # RStudio project file\n|\n├─ README.md                                # Presentation of the project\n├─ DESCRIPTION                              # Project metadata\n├─ LICENSE.md                               # License of the project\n|\n├─ data/                                    # Contains raw data\n|  └─ pantheria/                            # PanTHERIA database\n|     └─ PanTHERIA_1-0_WR05_Aug2008.txt\n|\n├─ outputs/                                 # Contains results\n├─ figures/                                 # Contains figures\n|\n├─ R/                                       # Contains R functions (only)\n|  └─ dl_pantheria_data.R                   # Function to download PanTHERIA data\n|\n├─ analyses/                                # Contains R scripts\n|  └─ download-data.R                       # Script to download raw data\n|\n└─ make.R                                   # Script to setup & run the project\n\n\n## Installation\n\nComing soon...\n\n\n## Usage\n\nOpen the `practice.Rproj` file in RStudio and run `source(\"make.R\")` to launch \nanalyses. \n\n- All packages will be automatically installed and loaded\n- Datasets will be saved in the `data/` directory\n\n\n## License\n\nThis project is released under the \n[GPL-3](https://choosealicense.com/licenses/gpl-3.0/) license.\n\n\n## Citation\n\n&gt; Doe J (2024) Download PanTHERIA and WWF WildFinder databases.\n\n\n## References\n\nJone KE, Bielby J, Cardillo M _et al._ (2009) PanTHERIA: A \nspecies-level database of life history, ecology, and geography of extant and \nrecently extinct mammals. _Ecology_, 90, 2648. \nDOI: [10.1890/08-1494.1](https://doi.org/10.1890/08-1494.1)\n\n\n\nCongratulations \nYour project is now a functional and reproducible research compendium.\n\n\n  The final compendium can be found here.\n\n\n\n\n\n\n\nNoteThe rcompendium::new_compendium() function\n\n\n\nAll these steps can be performed with a single function: new_compendium() from rcompendium. Read the documentation carefully before using this function."
  },
  {
    "objectID": "posts/2024-09-24-research-compendium/index.html#references",
    "href": "posts/2024-09-24-research-compendium/index.html#references",
    "title": "Research compendium",
    "section": "References",
    "text": "References\nJones KE, Bielby J, Cardillo M et al. (2009) PanTHERIA: a species-level database of life history, ecology, and geography of extant and recently extinct mammals. Ecology, 90, 2648. DOI: https://doi.org/10.1890/08-1494.1.\nMarwick B, Boettiger C & Mullen L (2018) Packaging data analytical work reproducibly using R (and friends). PeerJ. DOI: https://doi.org/10.7287/peerj.preprints.3192v2."
  },
  {
    "objectID": "contribute.html",
    "href": "contribute.html",
    "title": "Contribute",
    "section": "",
    "text": "First off, thanks for taking the time to contribute to tips-and-tricks! All types of contributions are encouraged and valued.\nIf you want to contribute by writing your own post, this page is for you. Read the CONTRIBUTING.md guide for any other types of contribution (reporting bug, fixing typo, improving published post, etc.)."
  },
  {
    "objectID": "contribute.html#workflow",
    "href": "contribute.html#workflow",
    "title": "Contribute",
    "section": "Workflow",
    "text": "Workflow\nThis blog is hosted on GitHub and we use the GitHub flow to collaborate on this project.\nProceed as follow to write and submit your own post:\n\nFork this repository using the GitHub interface.\nClone your fork by opening RStudio IDE and create a New Project from Version Control.\nSet up and write your post (see below Writing a post).\nStage (git add) and commit (git commit) your changes.\nPush your changes to your GitHub fork.\nSubmit a Pull Request on the original repo (see below Pull request).\n\nWe will then review your Pull Request as soon as possible.\n\n\n\n\n\n\nImportantBranch naming rule\n\n\n\nFor each post, a new git branch will be created automatically and named as follow (in lower case): yyyy-mm-dd-title-of-the-post. This will make easier the review process and the maintenance of this blog. For example: 2024-05-07-working-with-git-branches."
  },
  {
    "objectID": "contribute.html#requirements",
    "href": "contribute.html#requirements",
    "title": "Contribute",
    "section": "Requirements",
    "text": "Requirements\nHere is a list of the software required to contribute to this blog:\n\n\n\nSoftware\nDescription\nWebsite\n\n\n\n\nR\nThe R environment\nlink\n\n\nRStudio Desktop\nIntegrated development environment (IDE) for R\nlink\n\n\nQuarto CLI\nScientific publishing system used for this blog\nlink\n\n\nGit\nVersion control software\nlink\n\n\n\n In addition:\n\nInstall the  packages cli, devtools, and gert.\nCheck your git configuration by reading this dedicated page."
  },
  {
    "objectID": "contribute.html#authoring",
    "href": "contribute.html#authoring",
    "title": "Contribute",
    "section": "Authoring",
    "text": "Authoring\n\nPost setup\nRun the  command devtools::load_all() to load and access helper functions developed for this blog.\nThen create the file structure for your new post. The function create_post() has been developed to make this task easier.\nFor example,\n\n## Create a new post ----\ncreate_post(title = \"Working with git branches\", date = \"2024-05-07\")\n\nN.B. You can omit the argument date if you want to use the today date.\nThis function will:\n\ncreate a new git branch named 2024-05-07-working-with-git-branches\nswitch the repo to this new git branch\ncreate a subfolder in posts/ named 2024-05-07-working-with-git-branches\ncreate a Quarto file in this subfolder named index.qmd\n\nYou will write the content of your post inside this index.qmd file and add additional files (images, data, etc.) in the subfolder posts/2024-05-07-working-with-git-branches/.\n\n\nPost metadata\nBefore starting writing your post, please edit the post metadata (i.e. the YAML section) in the post file (index.qmd file) as follow:\n---\ntitle: \"Working with git branches\"\nauthor: \"Nicolas Casajus\"\ndate: \"2024-05-07\"\ncategories: [git, git-branch, git-checkout, git-merge]\ntoc: true\ndraft: false\nlightbox: true\ncode-overflow: scroll\n---\n\nAdd your name in the author field.\nAdd 3 to 5 tags in the categories field. They will be used to feed to search engine.\n\n\n\nPost content\nThis blog uses the Markdown syntax (Pandoc flavor). By running in the terminal the Quarto command quarto preview, you will start a live server to preview your post as you type."
  },
  {
    "objectID": "contribute.html#pull-request",
    "href": "contribute.html#pull-request",
    "title": "Contribute",
    "section": "Pull request",
    "text": "Pull request\nA Pull request is a proposal to merge a set of changes from one branch into another (GitHub documentation).\nTo submit your new post, you will create a Pull request (PR) to merge the branch 2024-05-07-working-with-git-branches of your fork into the branch main of the original repo.\nFirst, send your local changes to your GitHub fork. On a terminal, run the following command to push your new branch:\n## Send your new branch to GitHub (fork) ----\ngit push --set-upstream origin 2024-05-07-working-with-git-branches\nThen create a new PR by visiting your GitHub fork homepage: you should see a new green button named Compare & Pull request (Fig. 1).\n\n\n\n\n\n\n\nFigure 1. Create a Pull request from a Fork\n\n\nAdd some information about your PR (Fig. 2):\n\nCheck that you have selected the appropriate repos and branches (A).\nAdd a title (B).\nAdd a description (C). You can link this PR to an existing Issue.\n\n\n\n\n\n\n\n\nFigure 2. Draft a Pull request\n\n\nCreate a Draft Pull request (D): this will allow you to keep working on your post (i.e. adding commits). All new commits on the branch 2024-05-07-working-with-git-branches will be automatically added to this PR.\nOnce your post is ready, click on Ready for review to finalize your PR (Fig. 3).\n\n\n\n\n\n\n\nFigure 3. Create a Pull request\n\n\nThis will open a new PR and inform the blog maintainers that they can review your PR.\n\n\n\n\n\n\n\nFigure 4. Pull request ready for review\n\n\nYour post will be merged (by the blog maintainer) into the main branch and will be accessible a few minutes later here."
  }
]