{
  "hash": "9755bf280d25bab9c9d6d710f1d45e50",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Parallel Computing in R\"\nauthor: \"Nicolas Casajus\"\ndate: \"2025-01-28\"\ncategories: [r, parallel-computing, spatial, optimization]\nimage: \"\"\ntoc: true\ndraft: false\nlightbox: true\ncode-overflow: scroll\n---\n\n\n## Introduction\n\nTraditionally, when we work with {{< fa brands r-project >}} we use the **sequential computing** approach where instructions are processed one at a time, with each subsequent instruction waiting for the previous one to complete. It typically uses a **single processor**, which can result in **lower performance** and higher processor workload. The primary drawback of sequential computing is that it can be **time-consuming**, as only one instruction is executed at any given moment.\n\n**Parallel computing** was introduced to overcome these limitations. In this approach, multiple instructions or processes are executed concurrently. By running tasks simultaneously, parallel computing **saves time** and is capable of **solving larger problems**. It uses **multiple high-performance processors**, which results in a lower workload for each processor.\n\n\n<br/>\n\n### What is a CPU?\n\nThe **Central Processing Unit** (**CPU**), also called processor, is a piece of computer hardware (electronic circuitry) that executes instructions, such as arithmetic, logic, controlling, and input/output (I/O) operations[^wiki]. In other words, it is the brain of the computer.\n\nModern CPUs consist of several units, named (**physical**) **cores** (multi-core processors). These cores can be **multithreaded**. This means that these cores can provide multiple threads of execution in parallel (usually up to 2). Threads are also known as **logical cores**. \n\n[^wiki]: <https://en.wikipedia.org/wiki/Central_processing_unit>\n\n<br/>\n\n::::{.columns}\n:::{.column width=30%}\n![](cpu.png){width=90%}\n:::\n:::{.column width=70%}\nIn this example, the CPU (in brown) contains 4 cores (in blue and green):\n\n- 2 single-threading cores (in blue)\n- 2 multi-threading cores (in green)\n\nSix threads (logical cores) are available for parallel computing.\n:::\n::::\n\n::: {.callout-note}\nFor instance, my [Lenovo P14s Gen 3](https://www.lenovo.com/fr/fr/p/laptops/thinkpad/thinkpadp/thinkpad-p14s-gen-3-(14-inch-intel)/len101t0011) is shipped with the [Intel&reg; Core&trade; i7-1280P processor](https://www.intel.com/content/www/us/en/products/sku/226253/intel-core-i71280p-processor-24m-cache-up-to-4-80-ghz/specifications.html). It is made up of **6 multi-threading cores** (2 threads per core) and **8 single-threading cores**. Parallel computation can use **20 threads**.\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# How many logical cores are available on my computer?\nparallel::detectCores(logical = TRUE)\n## [1] 20\n```\n:::\n\n\n\n<br/>\n\n### Parallel computing\n\nHere we will focus on the [**Embarrassingly parallel computing paradigm**](https://en.wikipedia.org/wiki/Embarrassingly_parallel). In this paradigm, a problem can be split into multiple independent pieces. The pieces of the problem are executed simultaneously as they don't have to communicate with each other (except at the end when all outputs are assembled).\n\n\n::: {.callout-important}\n## Hidden parallel tasks\n\nMany {{< fa brands r-project >}} packages (and system libraries) include built-in parallel computing that operates behind the scenes. This **hidden parallel computing** won't interfere with your work and will actually enhance computational efficiency. However, it's still a good idea to be aware of it, as it could impact other tasks (and users) using the same machine.\n:::\n\n{{< fa hand-point-right >}}&nbsp; **Data parallel computing** involves splitting a large dataset into smaller segments and performing the same operation on each segment simultaneously. This approach is especially useful for tasks where the same computation must be applied to multiple data points (e.g. large data processing, simulations, machine learning, image and video processing).\n\n\nMany {{< fa brands r-project >}} packages implement high-performance and parallel computing (see this [CRAN Task View](https://cran.r-project.org/web/views/HighPerformanceComputing.html)).\n\nThe `parallel` package in {{< fa brands r-project >}} implements two types of parallel computing:\n\n- **Shared memory parallelization** (or **`FORK`**): Each parallel thread is essentially a full copy of the master process, along with the shared environment, including objects and variables defined before the parallel threads are launched. This makes it run efficiently, especially when handling large datasets. However, a key limitation is that the approach does not work on Windows systems.\n- **Distributed memory parallelization** (or **`SOCKET`**): Each thread operates independently, without sharing objects or variables, which must be explicitly passed from the master process. As a result, it tends to run slower due to the overhead of communication. This approach is compatible with all operating systems.\n\n|                     | Forking          | Socket                   |\n|:--------------------|:-----------------|:--------------------------|\n| **Operating system** | {{< fa brands apple >}} &nbsp; {{< fa brands ubuntu >}} | {{< fa brands windows >}} &nbsp; {{< fa brands apple >}} &nbsp; {{< fa brands ubuntu >}} |\n| **Environment**      | Common to all sessions<br/>(time saving) | Unique to each session<br/>(transfer variables, functions & packages)     |\n| **Usage**            | Very easy | More difficult<br/>(configure cluster & environment) |\n\n: Table: Comparison of two parallel computing approaches\n\n:::{.small}\n_Source (in French): <https://regnault.pages.math.cnrs.fr/meroo/src/quarto/calc_paral_R.html>_\n:::\n\nLet's explore these two approaches with a case study.\n\n\n<br/>\n\n\n\n\n## Case study\n\n{{< fa hand-point-right >}}&nbsp; **Objective**: We want to intersect the spatial distribution of 40 species on a spatial grid (defining the Western Palearctic region) to create a matrix with grid cells in row and the presence/absence of each species in column.\n\nThe workflow is the following:\n\n1. Import study area (grid)\n2. Import species spatial distributions (polygons)\n3. Subset polygons for species X\n4. Intersect polygons X with the grid\n5. Assemble occurrences of all species\n\nSteps 3-4 must be repeated for each species and therefore can be parallelized.\n\n\n<br/>\n\n### Import data\n\nLet's import the study area, a regular spatial grid (`sf` `POLYGONS`) defined in the WGS84 system and delimiting the Western Palearctic region.\n\n\n::: {.cell}\n\n:::\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Import study area ----\nstudy_area  <- sf::st_read(file.path(\"data\", \"study_area.gpkg\"))\nstudy_area\n```\n:::\n\n\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\nSimple feature collection with 3254 features and 1 field\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -24.53429 ymin: 14.91196 xmax: 69.02288 ymax: 81.85871\nGeodetic CRS:  WGS 84\nFirst 10 features:\n   id                           geom\n1   1 MULTIPOLYGON (((11.46571 15...\n2   2 MULTIPOLYGON (((11.46571 15...\n3   3 MULTIPOLYGON (((12.46571 15...\n4   4 MULTIPOLYGON (((13.46571 15...\n5   5 MULTIPOLYGON (((14.46571 15...\n6   6 MULTIPOLYGON (((15.46571 15...\n7   7 MULTIPOLYGON (((16.46571 15...\n8   8 MULTIPOLYGON (((17.46571 15...\n9   9 MULTIPOLYGON (((26.46571 15...\n10 10 MULTIPOLYGON (((26.46571 15...\n```\n\n\n:::\n:::\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Attach ggplot2 ----\nlibrary(\"ggplot2\")\n\n# Map study area ----\nggplot() +\n  theme_bw() +\n  theme(text = element_text(family = \"serif\")) +\n  geom_sf(data = study_area)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/map-area-1.png){width=672}\n:::\n:::\n\n\nNow let's import the bird spatial distribution layer (`sf` `POLYGONS`) also defined in the WGS84 system.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Import species distribution ----\nsp_polygons <- sf::st_read(file.path(\"data\", \"species_polygons.gpkg\"))\nsp_polygons\n```\n:::\n\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\nSimple feature collection with 40 features and 1 field\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -30 ymin: 10 xmax: 75 ymax: 81.85748\nGeodetic CRS:  WGS 84\nFirst 10 features:\n     binomial                           geom\n1  species_01 MULTIPOLYGON (((6.04248 45....\n2  species_02 MULTIPOLYGON (((-16.16014 2...\n3  species_03 MULTIPOLYGON (((29.5531 40....\n4  species_04 MULTIPOLYGON (((75 12.38982...\n5  species_05 MULTIPOLYGON (((-7.675472 4...\n6  species_06 MULTIPOLYGON (((-3.947759 4...\n7  species_07 MULTIPOLYGON (((4.209473 36...\n8  species_08 MULTIPOLYGON (((75 42.27599...\n9  species_09 MULTIPOLYGON (((-4.051088 4...\n10 species_10 MULTIPOLYGON (((5.534302 61...\n```\n\n\n:::\n:::\n\n\n<br/>\n\n### Extract species names\n\nLet's extract the name of the 40 species (used later for parallel computing).\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Extract species names ----\nsp_names <- unique(sp_polygons$\"binomial\")\nsp_names <- sort(sp_names)\nsp_names\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] \"species_01\" \"species_02\" \"species_03\" \"species_04\" \"species_05\"\n [6] \"species_06\" \"species_07\" \"species_08\" \"species_09\" \"species_10\"\n[11] \"species_11\" \"species_12\" \"species_13\" \"species_14\" \"species_15\"\n[16] \"species_16\" \"species_17\" \"species_18\" \"species_19\" \"species_20\"\n[21] \"species_21\" \"species_22\" \"species_23\" \"species_24\" \"species_25\"\n[26] \"species_26\" \"species_27\" \"species_28\" \"species_29\" \"species_30\"\n[31] \"species_31\" \"species_32\" \"species_33\" \"species_34\" \"species_35\"\n[36] \"species_36\" \"species_37\" \"species_38\" \"species_39\" \"species_40\"\n```\n\n\n:::\n:::\n\n\n<br/>\n\n### Explore data\n\nWe will select polygons for **species_22** and map layers.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Subset one species ----\nspecies      <- \"species_22\"\nsub_polygons <- sp_polygons[sp_polygons$\"binomial\" == species, ]\nsub_polygons\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSimple feature collection with 1 feature and 1 field\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -6.235596 ymin: 52.44647 xmax: 34.74487 ymax: 71.18811\nGeodetic CRS:  WGS 84\n     binomial                           geom\n22 species_22 MULTIPOLYGON (((5.123108 60...\n```\n\n\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Map layers ----\nggplot() +\n  theme_bw() +\n  theme(text = element_text(family = \"serif\")) +\n  geom_sf(data = study_area) +\n  geom_sf(data = sub_polygons, \n          fill = \"#4F000088\", \n          col  = \"#4F0000FF\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/map-data-1.png){width=672}\n:::\n:::\n\n\n\n\n<br/>\n\n### Define main function\n\nNow let's define a function that will report the occurrence of one species on the grid (step 4 of the workflow).\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Function to rasterize ----\npolygon_to_grid <- function(grid, polygon) {\n\n  ## Clean species name ----\n  species <- unique(polygon$\"binomial\")\n  species <- gsub(\" \", \"_\", species) |> \n    tolower()\n  \n  ## Intersect layers ----\n  cells <- sf::st_intersects(grid, polygon, sparse = FALSE)\n  cells <- apply(cells, 1, any)\n  cells <- which(cells)\n  \n  ## Create presence/absence column ----\n  grid[     , species] <- 0\n  grid[cells, species] <- 1\n  \n  ## Remove spatial information ----\n  sf::st_drop_geometry(grid)\n}\n```\n:::\n\n\nThis function returns a `data.frame` with two columns: the identifier of the cell and the presence/absence of the species.\n\n<br/>\n\n### Test main function\n\nLet's try this function with the distribution of _Curruca cantillans_.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Rasterize species polygons -----\nsp_grid <- polygon_to_grid(study_area, sub_polygons)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nalthough coordinates are longitude/latitude, st_intersects assumes that they\nare planar\n```\n\n\n:::\n\n```{.r .cell-code}\nhead(sp_grid)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  id species_22\n1  1          0\n2  2          0\n3  3          0\n4  4          0\n5  5          0\n6  6          0\n```\n\n\n:::\n:::\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Add geometry ----\nsf::st_geometry(sp_grid) <- sf::st_geometry(study_area)\n\n# Convert occurrence to factor ----\nsp_grid$\"species_22\" <- as.factor(sp_grid$\"species_22\")\n\n# Map result ----\nggplot() +\n  theme_bw() +\n  theme(text = element_text(family = \"serif\"), legend.position = \"none\") +\n  geom_sf(data    = sp_grid, \n          mapping = aes(fill = species_22)) +\n  scale_fill_manual(values = c(\"0\" = \"#FFFFFFFF\", \n                               \"1\" = \"#9F0000FF\"))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/map-grid-1.png){width=672}\n:::\n:::\n\n\n\nOur workflow is ready for one species. Now we have to apply this function on each species. Let's have a look at different approaches.\n\n\n<br/>\n\n### Non-parallel: `for()` loop\n\nOne way of repeating a task is the **iteration**. Let's walk over species with the `for()` loop.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create the output (list of 40 empty elements) ----\ngrids <- vector(mode = \"list\", length = length(sp_names))\n\n# Define the sequence to loop on ----\nfor (i in 1:length(sp_names)) {\n  \n  # Instructions to repeat ----\n  tmp <- sp_polygons[sp_polygons$\"binomial\" == sp_names[i], ]\n  grd <- polygon_to_grid(study_area, tmp)\n  \n  # Store result in the output ----\n  grids[[i]] <- grd\n\n}\n```\n:::\n\n\nThe output is a list of 40 `data.frame`.\n\n\n<br/>\n\n### Non-parallel: `lapply()`\n\nIterative computing is quite verbose and often time-consuming. Because {{< fa brands r-project >}} is a functional programming language, we can wrap-up the body of the `for()` loop inside a function and apply this function over a vector (i.e. species names). Let's illustrate this with the function `lapply()`.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# lapply way ----\ngrids <- lapply(sp_names, function(sp) {\n  \n  tmp <- sp_polygons[sp_polygons$\"binomial\" == sp, ]\n  grd <- polygon_to_grid(study_area, tmp)\n  grd\n\n})\n```\n:::\n\n\nThe function `lapply()` also returns a list of 40 `data.frame`.\n\n\n::: {.callout-tip}\n## The `purrr` package\n\nThe [`purrr`](https://purrr.tidyverse.org/) package provides an alternative to the `apply()` function family in base {{< fa brands r-project >}}. For instance:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# purrr way ----\ngrids <- purrr::map(sp_names, function(sp) {\n  \n  tmp <- sp_polygons[sp_polygons$\"binomial\" == sp, ]\n  grd <- polygon_to_grid(study_area, tmp)\n  grd\n\n})\n```\n:::\n\n\nThe `map()` function of the `purrr` package returns the same output as `lapply()`.\n:::\n\nThese three approaches work in a sequential way. It's time to parallel our code to boost performance.\n\n\n<br/>\n\n### Forking: `mclapply()`\n\nThe `parallel` package provides the function `mclapply()`, a parallelized version of `lapply()`. Note that this function relies on the forking approach and doesn't work on Windows.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Required packages ----\nlibrary(parallel)\n\n# Let's use 10 threads ----\nn_cores <- 10\n\n# mclapply way ----\ngrids <- mclapply(sp_names, function(sp) {\n  \n  tmp <- sp_polygons[sp_polygons$\"binomial\" == sp, ]\n  grd <- polygon_to_grid(study_area, tmp)\n  grd\n\n}, mc.cores = n_cores)\n```\n:::\n\n\nThe argument `mc.cores` is used to indicate the number of threads to use.\nThe `mclapply()` will always return a list (as `lapply()`).\n\n\n{{< fa hand-point-right >}}&nbsp; If you are on Unix-based systems (GNU/Linux or macOS), you should use the **forking** approach.\n\n\n<br/>\n\n### Socket: `parLapply()`\n\nThe **socket** approach is available for all operating systems (including Windows). The `parLapply()` function is the equivalent to the `mclapply()` but additional actions are required to parallelize code under the socket approach:\n\n- Create a socket with the required number of threads with `makeCluster()`\n- Transfer the environment: packages with `clusterEvalQ()` and variables/functions in memory with `clusterExport()`\n- Stop the socket at the end of the parallel computation with `stopCluster()`\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Required packages ----\nlibrary(parallel)\n\n# Let's use 10 threads ----\nn_cores <- 10\n\n# Create a socket w/ 10 threads ----\ncluster <- makeCluster(spec = n_cores)\n\n# Attach packages in each session ----\nclusterEvalQ(cl   = cluster, \n             expr = { \n  library(sf) \n})\n\n# Transfer data & functions to each session ----\nclusterExport(cl      = cluster, \n              varlist = c(\"sp_names\", \"sp_polygons\", \"study_area\", \n                          \"polygon_to_grid\"),\n              envir   = environment())\n\n# Parallel computing ----\ngrids <- parLapply(cl = cluster, X = sp_names, function(sp) {\n  \n  tmp <- sp_polygons[sp_polygons$\"binomial\" == sp, ]\n  grd <- polygon_to_grid(study_area, tmp)\n  grd\n  \n})\n\n# Stop socket ----\nstopCluster(cluster)\n```\n:::\n\n\nThe `parLapply()` also returns a list of 40 `data.frame`.\n\n\n\n<br/>\n\n### Socket: `foreach()`\n\nInstead of the `parLapply()` you can use the `foreach()` function of the `foreach` package. But before parallelizing the code, you will need to register the socket with the `registerDoParallel()` function of the package `doParallel`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Required packages ----\nlibrary(parallel)\nlibrary(foreach)\nlibrary(doParallel)\n\n# Let's use 10 threads ----\nn_cores <- 10\n\n# Create a socket w/ 10 threads ----\ncluster <- makeCluster(spec = n_cores)\n\n# Attach packages in each session ----\nclusterEvalQ(cl   = cluster, \n             expr = { \n  library(sf) \n})\n\n# Transfer data & functions to each session ----\nclusterExport(cl      = cluster, \n              varlist = c(\"sp_names\", \"sp_polygons\", \"study_area\", \n                          \"polygon_to_grid\"),\n              envir   = environment())\n\n# Register socket ----\nregisterDoParallel(cluster)\n\n# Parallel computing ----\ngrids <- foreach(i = 1:length(sp_names), .combine = list) %dopar% {\n  \n  tmp <- sp_polygons[sp_polygons$\"binomial\" == sp_names[i], ]\n  grd <- polygon_to_grid(study_area, tmp)\n  grd\n}\n\n# Stop socket ----\nstopCluster(cluster)\n```\n:::\n\n\nThe argument `.combine` of the `foreach()` function can be used to specify how the individual results are combined together. By default the function returns a `list`.\n\n\n\n<br/>\n\n### Benchmark\n\nLet's compare the speed of each methods with the `system.time()` function.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Required packages ----\nlibrary(parallel)\nlibrary(foreach)\nlibrary(doParallel)\n\n# Let's use 10 threads ----\nn_cores <- 10\n\n# for loop ----\nfor_loop <- system.time({\n  grids <- vector(mode = \"list\", length = length(sp_names))\n  for (i in 1:length(sp_names)) {\n    tmp <- sp_polygons[sp_polygons$\"binomial\" == sp_names[i], ]\n    grids[[i]] <- polygon_to_grid(study_area, tmp)\n  }\n})\n\n\n# lapply way ----\nlapply_way <- system.time({\n  lapply(sp_names, function(sp) {\n    tmp <- sp_polygons[sp_polygons$\"binomial\" == sp, ]\n    polygon_to_grid(study_area, tmp)\n  })\n})\n\n\n# purrr way ----\npurrr_way <- system.time({\n  purrr::map(sp_names, function(sp) {\n    tmp <- sp_polygons[sp_polygons$\"binomial\" == sp, ]\n    polygon_to_grid(study_area, tmp)\n  })\n})\n\n\n# mclapply way ----\nmclapply_way <- system.time({\n  mclapply(sp_names, function(sp) {\n    tmp <- sp_polygons[sp_polygons$\"binomial\" == sp, ]\n    polygon_to_grid(study_area, tmp)\n  }, \n  mc.cores = n_cores)\n})\n\n\n# parlapply way ----\nparlapply_way <- system.time({\n  cluster <- makeCluster(spec = n_cores)\n  clusterEvalQ(cl = cluster, expr = { library(sf) })\n  clusterExport(cl      = cluster, \n                varlist = c(\"sp_names\", \"sp_polygons\", \n                            \"study_area\", \"polygon_to_grid\"),\n                envir   = environment())\n  parLapply(cl = cluster, sp_names, function(sp) {\n    tmp <- sp_polygons[sp_polygons$\"binomial\" == sp, ]\n    polygon_to_grid(study_area, tmp)\n  })\n  stopCluster(cluster)\n})\n\n\n# foreach way ----\nforeach_way <- system.time({\n  cluster <- makeCluster(spec = n_cores)\n  clusterEvalQ(cl = cluster, expr = { library(sf) })\n  clusterExport(cl      = cluster, \n                varlist = c(\"sp_names\", \"sp_polygons\", \"study_area\", \n                            \"polygon_to_grid\"),\n                envir   = environment())\n  registerDoParallel(cluster)\n  foreach(i = 1:length(sp_names), .combine = list) %dopar% {\n    tmp <- sp_polygons[sp_polygons$\"binomial\" == sp_names[i], ]\n    polygon_to_grid(study_area, tmp)\n  }\n  stopCluster(cluster)\n})\n\n\n# Benchmark ----\nrbind(for_loop, lapply_way, purrr_way, \n      mclapply_way, parlapply_way, foreach_way)\n```\n:::\n\n\n```\n              elapsed\nfor_loop       12.488\nlapply_way     12.369\npurrr_way      12.367\nforeach_way     3.650\nparlapply_way   3.348\nmclapply_way    2.246\n```\n\nAs we can see, parallelizing portions of code is very efficient. In this example, both **fork** and **socket** approaches are quite fast. But differences in performance may appear depending on your code (and your available memory).\n\n<br/>\n\n### Bonus: aggregate outputs\n\nFinally, let's aggregate all `data.frame` stored in the `list` into a single spatial object.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Parallel computing (forking) ----\ngrids <- parallel::mclapply(sp_names, function(sp) {\n  \n  tmp <- sp_polygons[sp_polygons$\"binomial\" == sp, ]\n  polygon_to_grid(study_area, tmp)\n  \n}, mc.cores = n_cores)\n\n# Aggregation ----\ngrids <- do.call(cbind, grids)\n\n# Keep only species columns ----\ngrids <- grids[ , which(colnames(grids) != \"id\")]\n\n# Convert to spatial object ----\nsf::st_geometry(grids) <- sf::st_geometry(study_area)\n```\n:::\n\n\n\n<br/>\n\n### To go further\n\n- Introduction to parallel computing with R - [link](https://rawgit.com/PPgp/useR2017public/master/tutorial.html)\n- Calcul parallèle avec R [in French] - [link](https://regnault.pages.math.cnrs.fr/meroo/src/quarto/calc_paral_R.html)\n- Boosting R performance with parallel processing package `snow` - [link](https://sabarevictor.medium.com/boosting-r-performance-with-parallel-processing-pacakge-snow-d638b7a6a37d)\n- `future`: Unified parallel and distributed processing in R for everyone - [link](https://future.futureverse.org/)\n- Quick introduction to parallel computing in R - [link](https://nceas.github.io/oss-lessons/parallel-computing-in-r/parallel-computing-in-r.html)\n- Paralléliser R [in French] - [link](https://ericmarcon.github.io/travailleR/chap-utiliseR.html#sec:parallel)\n- Parallel computation -  [link](https://bookdown.org/rdpeng/rprogdatascience/parallel-computation.html)\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}